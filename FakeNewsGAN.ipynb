{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras import Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, Conv1D, Dropout, Dense, Embedding, Flatten, Reshape, Multiply, Lambda, UpSampling1D, MaxPooling1D, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Traitement des données</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement des deux tables\n",
    "real_news = pd.read_csv(\"./dataset1/True.csv\")\n",
    "fake_news = pd.read_csv(\"./dataset1/Fake.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vrai = 1, faux = 0\n",
    "real_news[\"label\"] = 1\n",
    "fake_news[\"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20526</th>\n",
       "      <td>New Kosovo PM pledges dialogue with Serbia, gr...</td>\n",
       "      <td>PRISTINA (Reuters) - Kosovo s newly-elected pr...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 9, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5125</th>\n",
       "      <td>Trump administration to propose 'dramatic redu...</td>\n",
       "      <td>WASHINGTON (Reuters) - The White House budget ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>March 4, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9178</th>\n",
       "      <td>IT JUST GOT REAL! GOP Rep. Jim Jordan Tells Ju...</td>\n",
       "      <td>One of the big players in trying to get to the...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Dec 17, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15242</th>\n",
       "      <td>WOW! DONALD TRUMP HAMMERS OBAMA ON HIS “Terrib...</td>\n",
       "      <td>BRAVO and spot on!Click on picture below to wa...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Sep 2, 2015</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7328</th>\n",
       "      <td>Michele Bachmann Believes God Was Punishing O...</td>\n",
       "      <td>While most of us probably wish former Congress...</td>\n",
       "      <td>News</td>\n",
       "      <td>March 23, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14364</th>\n",
       "      <td>Zimbabwe broadcaster on stand-by for address b...</td>\n",
       "      <td>HARARE (Reuters) - Zimbabwe s state broadcaste...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>November 20, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10959</th>\n",
       "      <td>LAS VEGAS: ILLEGAL ALIEN ARRESTED For Filming ...</td>\n",
       "      <td>Did you know that in the state of Texas alone ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>May 6, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20947</th>\n",
       "      <td>CONGRESS JUST DEALT A BIG BLOW To Obama And Hi...</td>\n",
       "      <td>Obama has shown favoritism towards the Muslim ...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Feb 26, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8507</th>\n",
       "      <td>Icahn praises Trump economic plan, says candid...</td>\n",
       "      <td>NEW YORK (Reuters) - Billionaire activist inve...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>August 9, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7642</th>\n",
       "      <td>Clinton leads Trump 42 to 36 percent as he los...</td>\n",
       "      <td>NEW YORK (Reuters) - Democrat Hillary Clinton’...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>October 28, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "20526  New Kosovo PM pledges dialogue with Serbia, gr...   \n",
       "5125   Trump administration to propose 'dramatic redu...   \n",
       "9178   IT JUST GOT REAL! GOP Rep. Jim Jordan Tells Ju...   \n",
       "15242  WOW! DONALD TRUMP HAMMERS OBAMA ON HIS “Terrib...   \n",
       "7328    Michele Bachmann Believes God Was Punishing O...   \n",
       "14364  Zimbabwe broadcaster on stand-by for address b...   \n",
       "10959  LAS VEGAS: ILLEGAL ALIEN ARRESTED For Filming ...   \n",
       "20947  CONGRESS JUST DEALT A BIG BLOW To Obama And Hi...   \n",
       "8507   Icahn praises Trump economic plan, says candid...   \n",
       "7642   Clinton leads Trump 42 to 36 percent as he los...   \n",
       "\n",
       "                                                    text       subject  \\\n",
       "20526  PRISTINA (Reuters) - Kosovo s newly-elected pr...     worldnews   \n",
       "5125   WASHINGTON (Reuters) - The White House budget ...  politicsNews   \n",
       "9178   One of the big players in trying to get to the...      politics   \n",
       "15242  BRAVO and spot on!Click on picture below to wa...      politics   \n",
       "7328   While most of us probably wish former Congress...          News   \n",
       "14364  HARARE (Reuters) - Zimbabwe s state broadcaste...     worldnews   \n",
       "10959  Did you know that in the state of Texas alone ...      politics   \n",
       "20947  Obama has shown favoritism towards the Muslim ...     left-news   \n",
       "8507   NEW YORK (Reuters) - Billionaire activist inve...  politicsNews   \n",
       "7642   NEW YORK (Reuters) - Democrat Hillary Clinton’...  politicsNews   \n",
       "\n",
       "                     date  label  \n",
       "20526  September 9, 2017       1  \n",
       "5125       March 4, 2017       1  \n",
       "9178         Dec 17, 2017      0  \n",
       "15242         Sep 2, 2015      0  \n",
       "7328       March 23, 2016      0  \n",
       "14364  November 20, 2017       1  \n",
       "10959         May 6, 2017      0  \n",
       "20947        Feb 26, 2016      0  \n",
       "8507      August 9, 2016       1  \n",
       "7642    October 28, 2016       1  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# création du dataset complet\n",
    "dataframe = pd.concat([real_news, fake_news])\n",
    "dataframe.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de références : 44898\n",
      "Nombre de fake news : 23481\n",
      "Nombre de vraies news : 21417\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nombre de références : {dataframe.title.count()}\")\n",
    "print(f\"Nombre de fake news : {fake_news.title.count()}\")\n",
    "print(f\"Nombre de vraies news : {real_news.title.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ici on ne s'intéresse qu'au titre et au label\n",
    "del dataframe[\"text\"]\n",
    "del dataframe[\"subject\"]\n",
    "del dataframe[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9883</th>\n",
       "      <td>WATCH: RACIST RAPPER WHO HUNG WHITE KID In Lat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>Conservatives ADMIT They Incited Violence At ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17518</th>\n",
       "      <td>WATCH: “IT’S THE MOST WONDERFUL TIME OF THE YE...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19860</th>\n",
       "      <td>SARAH JESSICA PARKER FEARS She’ll Be Attacked ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19707</th>\n",
       "      <td>CNN FIRES BLACK DEM Party Chair: New Wikileaks...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19391</th>\n",
       "      <td>China calls for restraint when asked about Nor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8592</th>\n",
       "      <td>U.S., Cuba hold 'substantive' second round tal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>Presidential Lawyer Comes Forward; Says Trump...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>Russian Investigation Landing Very Close To T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>U.S. lawmakers introduce bill to criminalize ‘...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  label\n",
       "9883   WATCH: RACIST RAPPER WHO HUNG WHITE KID In Lat...      0\n",
       "1774    Conservatives ADMIT They Incited Violence At ...      0\n",
       "17518  WATCH: “IT’S THE MOST WONDERFUL TIME OF THE YE...      0\n",
       "19860  SARAH JESSICA PARKER FEARS She’ll Be Attacked ...      0\n",
       "19707  CNN FIRES BLACK DEM Party Chair: New Wikileaks...      0\n",
       "19391  China calls for restraint when asked about Nor...      1\n",
       "8592   U.S., Cuba hold 'substantive' second round tal...      1\n",
       "893     Presidential Lawyer Comes Forward; Says Trump...      0\n",
       "1401    Russian Investigation Landing Very Close To T...      0\n",
       "8757   U.S. lawmakers introduce bill to criminalize ‘...      1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nettoyage des données (ponctuations)\n",
    "\n",
    "#stopWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "def cleanText(text):\n",
    "    forbidden = {\",\",\"@\",\";\",\"/\",\"-\",\":\",\".\",\"!\",\"?\", \"#\",\"\\\"\",\"(\",\")\",\"\\'\",\"’\",\"‘\",\"–\",\".\",\"&\"}\n",
    "    \n",
    "    if res is None:\n",
    "        return \"\"\n",
    "    res = str(text)\n",
    "    \n",
    "    for elm in forbidden:\n",
    "        res = res.replace(elm, \"\")\n",
    "    if len(res.split()) >= 30:\n",
    "        res = \" \".join(res.split()[0:30])\n",
    "    for elm in forbidden:\n",
    "        res = res.replace(\"  \", \" \")\n",
    "    return res\n",
    "\n",
    "#dataframe[\"title\"] = dataframe[\"title\"].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        As U.S. budget fight looms, Republicans flip t...\n",
       "1        U.S. military to accept transgender recruits o...\n",
       "2        Senior U.S. Republican senator: 'Let Mr. Muell...\n",
       "3        FBI Russia probe helped by Australian diplomat...\n",
       "4        Trump wants Postal Service to charge 'much mor...\n",
       "                               ...                        \n",
       "23476    McPain: John McCain Furious That Iran Treated ...\n",
       "23477    JUSTICE? Yahoo Settles E-mail Privacy Class-ac...\n",
       "23478    Sunnistan: US and Allied ‘Safe Zone’ Plan to T...\n",
       "23479    How to Blow $700 Million: Al Jazeera America F...\n",
       "23480    10 U.S. Navy Sailors Held by Iranian Military ...\n",
       "Name: title, Length: 44898, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données d'entrainement : 40408\n",
      "Données de test : 4490\n"
     ]
    }
   ],
   "source": [
    "# on sépare les données en données d'entraînement et données de test (80% et 20%)\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataframe[\"title\"], dataframe[\"label\"], test_size=0.10, random_state = 42)\n",
    "print(f\"Données d'entrainement : {len(x_train)}\")\n",
    "print(f\"Données de test : {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 2000 # taille max du vocab\n",
    "maxlen = 16 # taille max de séquence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorisation naïve en \"one-hot\"\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(dataframe[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorisation des données d'entraînement\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "#x_train = tokenizer.sequences_to_matrix(x_train)\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_oneHotSentence(sentence, maxfeatures):\n",
    "    oneHotSentence = []\n",
    "    for word in sentence:\n",
    "        oneHotWord = [0 for _ in range(maxfeatures)]\n",
    "        oneHotWord[word] = 1\n",
    "        oneHotSentence.append(oneHotWord)\n",
    "    return oneHotSentence\n",
    "\n",
    "def to_oneHotCorpus(corpus, maxfeatures):\n",
    "    oneHotCorpus = []\n",
    "    for sentence in corpus:\n",
    "        oneHotCorpus.append(to_oneHotSentence(sentence, maxfeatures))\n",
    "    return np.array(oneHotCorpus)\n",
    "\n",
    "def from_oneHotWord(oneHotWord):\n",
    "    for i in range(len(oneHotWord)):\n",
    "        if oneHotWord[i] > 0:\n",
    "            return i\n",
    "    return 0\n",
    "\n",
    "def from_oneHotSentence(oneHotSentence):\n",
    "    sentence = []\n",
    "    for oneHotWord in oneHotSentence:\n",
    "        sentence.append(from_oneHotWord(oneHotWord))\n",
    "    return sentence\n",
    "        \n",
    "def from_oneHotCorpus(oneHotCorpus):\n",
    "    return np.argmax(oneHotCorpus, axis=2)\n",
    "\n",
    "\n",
    "#x_train = to_oneHotCorpus(x_train, max_features)\n",
    "#x_train = from_oneHotCorpus(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorisation des données de test\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "#x_test = to_oneHotCorpus(x_test, max_features)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "nb_epochs = 20\n",
    "embedded_dim = 100\n",
    "latent_dim = 100\n",
    "kernel_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, nbpoints):\n",
    "    return np.random.uniform(0, 1, size=[nbpoints, latent_dim])\n",
    "\n",
    "def getLatentSamples(latent_dim, n):\n",
    "    labels = np.zeros(shape=n)\n",
    "    samples = generate_latent_points(latent_dim, n)\n",
    "    \n",
    "    return samples, labels\n",
    "\n",
    "def getFakeSamples(generator, latent_dim, n):\n",
    "    labels = np.ones(shape=n)\n",
    "    \n",
    "    latent_points = generate_latent_points(latent_dim, n)\n",
    "    samples = generator.predict(latent_points)\n",
    "    \n",
    "    return samples, labels\n",
    "\n",
    "def getRealSamples(X, Y, n):\n",
    "    random_indices = np.random.randint(0, X.shape[0], n)\n",
    "\n",
    "    samples = X[random_indices]\n",
    "    labels = Y[random_indices]\n",
    "    \n",
    "    return samples, labels\n",
    "\n",
    "def generateFakeNews(model, n):\n",
    "    few_points = generate_latent_points(latent_dim, n)\n",
    "    predictions = generator.predict(few_points)\n",
    "    fake_news = np.round(predictions)#np.argmax(predictions, axis=2)\n",
    "    fake_news = tokenizer.sequences_to_texts(fake_news)\n",
    "    \n",
    "    return fake_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(dim):\n",
    "    \n",
    "    input_layer = Input(shape=[dim])\n",
    "    \n",
    "    x = Dense(4 * 64, input_shape=[dim])(input_layer)\n",
    "    \n",
    "    x = Reshape((4, 64))(x)\n",
    "    \n",
    "    x = Conv1D(64, kernel_size, padding=\"same\", activation=\"softmax\")(x)\n",
    "    x = Conv1D(64, kernel_size, padding=\"same\", activation=\"softmax\")(x)\n",
    "    x = Conv1D(64, kernel_size, padding=\"same\", activation=\"softmax\")(x)\n",
    "    #x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = UpSampling1D()(x)\n",
    "    \n",
    "    x = Conv1D(128, kernel_size, padding=\"same\", activation=\"softmax\")(x)\n",
    "    x = Conv1D(128, kernel_size, padding=\"same\", activation=\"softmax\")(x)\n",
    "    x = Conv1D(128, kernel_size, padding=\"same\", activation=\"softmax\")(x)\n",
    "    #x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = UpSampling1D()(x)\n",
    "    \n",
    "    x = Conv1D(256, kernel_size, padding=\"same\", activation=\"softmax\")(x)\n",
    "    x = Conv1D(256, kernel_size, padding=\"same\", activation=\"softmax\")(x)\n",
    "    x = Conv1D(256, kernel_size, padding=\"same\", activation=\"softmax\")(x)\n",
    "    #x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = UpSampling1D()(x)\n",
    "    \n",
    "    x = Conv1D(512, kernel_size, padding=\"same\", activation=\"softmax\")(x)\n",
    "    x = Conv1D(512, kernel_size, padding=\"same\", activation=\"softmax\")(x)\n",
    "    x = Conv1D(512, kernel_size, padding=\"same\", activation=\"softmax\")(x)\n",
    "    #x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv1D(1024, kernel_size, padding=\"same\", activation=\"softmax\")(x)\n",
    "    x = Conv1D(1024, kernel_size, padding=\"same\", activation=\"softmax\")(x)\n",
    "    #x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(1024)(x)\n",
    "    \n",
    "    x = Dense(maxlen, activation=\"softmax\")(x)\n",
    "    \n",
    "    output_layer = Lambda(lambda x: x * max_features)(x)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.0005, beta_1=.5))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_discriminator():\n",
    "\n",
    "    input_layer = Input(shape=[maxlen])\n",
    "    \n",
    "    x = Lambda(lambda x: x / max_features)(input_layer)\n",
    "    \n",
    "    x = Dense(256)(x)\n",
    "    x = Reshape((256, 1))(x)\n",
    "    x = Conv1D(128, kernel_size, padding=\"same\")(x)\n",
    "    x = MaxPooling1D()(x)\n",
    "    x = Conv1D(64, kernel_size, padding=\"same\")(x)\n",
    "    x = MaxPooling1D()(x)\n",
    "    #x = Dropout(0.1)(x)\n",
    "    x = Conv1D(32, kernel_size, padding=\"same\")(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    output_layer = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.0002, beta_1=.5))\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_gan(generator, discriminator, latent_dim):\n",
    "    \n",
    "    input_layer = Input(shape=[latent_dim])\n",
    "    \n",
    "    x = generator(input_layer)\n",
    "    \n",
    "    output_layer = discriminator(x)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.0002, beta_1=.5))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               4352      \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 256, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 256, 128)          768       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 128, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 64, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 64, 32)            10272     \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 58,465\n",
      "Trainable params: 58,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "reshape_8 (Reshape)          (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 4, 64)             20544     \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 4, 64)             20544     \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 4, 64)             20544     \n",
      "_________________________________________________________________\n",
      "up_sampling1d_9 (UpSampling1 (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 8, 128)            41088     \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 8, 128)            82048     \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 8, 128)            82048     \n",
      "_________________________________________________________________\n",
      "up_sampling1d_10 (UpSampling (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 16, 256)           164096    \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 16, 256)           327936    \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 16, 256)           327936    \n",
      "_________________________________________________________________\n",
      "up_sampling1d_11 (UpSampling (None, 32, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 32, 512)           655872    \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 32, 512)           1311232   \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 32, 512)           1311232   \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 32, 1024)          2622464   \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 32, 1024)          5243904   \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1024)              33555456  \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                16400     \n",
      "_________________________________________________________________\n",
      "lambda_8 (Lambda)            (None, 16)                0         \n",
      "=================================================================\n",
      "Total params: 45,829,200\n",
      "Trainable params: 45,829,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = create_discriminator()\n",
    "generator = create_generator(latent_dim)\n",
    "gan = create_gan(generator, discriminator, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator_model, discriminator_model, gan_model, nb_epochs, batch_size):\n",
    "    discriminator_losses = []\n",
    "    \n",
    "    generator_losses = []\n",
    "    \n",
    "    for i in range(nb_epochs):\n",
    "\n",
    "        x_fake, y_fake = getFakeSamples(generator_model, latent_dim, batch_size)\n",
    "        x_real, y_real = getRealSamples(x_train, y_train, batch_size)\n",
    "        \n",
    "        discriminator_model.trainable = True\n",
    "\n",
    "        loss1 = discriminator_model.train_on_batch(x_real, y_real)\n",
    "        loss2 = discriminator_model.train_on_batch(x_fake, y_fake)\n",
    "        loss3 = (loss1 + loss2) / 2\n",
    "        \n",
    "        discriminator_model.trainable = False\n",
    "        \n",
    "        x_gan, y_gan = getLatentSamples(latent_dim, batch_size)        \n",
    "        \n",
    "        loss_gan = gan_model.train_on_batch(x_gan, y_gan)\n",
    "\n",
    "        discriminator_losses.append(loss3)\n",
    "        \n",
    "        generator_losses.append(loss_gan)\n",
    "        \n",
    "        print(f\"Epoch {i}  ;  Discriminator loss : {loss3}   ;    Generator loss : {loss_gan}\", end=\"\\r\")\n",
    "        if i % 100 == 0:\n",
    "            print(\"\\n\")\n",
    "            print(generateFakeNews(generator_model, 1))\n",
    "        \n",
    "    return discriminator_losses, generator_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shadow\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  ;  Discriminator loss : 0.6967480182647705   ;    Generator loss : 0.6891824007034302\n",
      "\n",
      "['all all all all all all all all all all all speech all democrat all all']\n",
      "Epoch 100  ;  Discriminator loss : 0.5360958576202393   ;    Generator loss : 1.0797394514083862\n",
      "\n",
      "[\"to to to u no u he iran's video to to to to to to\"]\n",
      "Epoch 200  ;  Discriminator loss : 0.5230628252029419   ;    Generator loss : 1.1512376070022583\n",
      "\n",
      "['just with to quake trump to']\n",
      "Epoch 300  ;  Discriminator loss : 0.522312343120575   ;    Generator loss : 1.13069736957550057\n",
      "\n",
      "['to of huge video on to fails to to']\n",
      "Epoch 400  ;  Discriminator loss : 0.5228368043899536   ;    Generator loss : 1.1469784975051884\n",
      "\n",
      "['she its turkey of for to after boiler warns in of to israel']\n",
      "Epoch 500  ;  Discriminator loss : 0.5402370691299438   ;    Generator loss : 1.1176868677139282\n",
      "\n",
      "['it hillary an u being brilliant obama says president shows time case goes u obama trump']\n",
      "Epoch 600  ;  Discriminator loss : 0.5293115377426147   ;    Generator loss : 1.10902023315429695\n",
      "\n",
      "['at is make obama no case immigration refugees show saudi meet have clinton shows hillary']\n",
      "Epoch 700  ;  Discriminator loss : 0.523077130317688   ;    Generator loss : 1.149310708045959563\n",
      "\n",
      "['vote from woman after crisis them support should so attorney obama’s it if him government']\n",
      "Epoch 800  ;  Discriminator loss : 0.5219103693962097   ;    Generator loss : 1.14031136035919226\n",
      "\n",
      "['vote not governor black america travel democrat american peace you while – deal anti up']\n",
      "Epoch 900  ;  Discriminator loss : 0.49950042366981506   ;    Generator loss : 1.1757403612136847\n",
      "\n",
      "['and what be you policy urges official democrat that migrants by obama watch that campaign']\n",
      "Epoch 1000  ;  Discriminator loss : 0.5236749649047852   ;    Generator loss : 1.1644951105117798\n",
      "\n",
      "['at against stupid black with no up will korea about puerto u cnn on a']\n",
      "Epoch 1100  ;  Discriminator loss : 0.5004349946975708   ;    Generator loss : 1.21149528026580895\n",
      "\n",
      "['white after a trump’s proves korea new white official media vote donald after second with']\n",
      "Epoch 1200  ;  Discriminator loss : 0.5134087800979614   ;    Generator loss : 1.20218420028686524\n",
      "\n",
      "['white gop his democrats after americans it’s free china hell from obama on senate house']\n",
      "Epoch 1300  ;  Discriminator loss : 0.4639306664466858   ;    Generator loss : 1.28410351276397738\n",
      "\n",
      "['obama black un just trump over with as obama with foundation in a to obama']\n",
      "Epoch 1400  ;  Discriminator loss : 0.4805673360824585   ;    Generator loss : 1.22613131999969484\n",
      "\n",
      "['on trump to is kills after at s rape is to says trump put trump']\n",
      "Epoch 1500  ;  Discriminator loss : 0.49132394790649414   ;    Generator loss : 1.2699341773986816\n",
      "\n",
      "['you democrats about top hillary hillary political be president community new s new clinton senate']\n",
      "Epoch 1600  ;  Discriminator loss : 0.5302246809005737   ;    Generator loss : 1.13895142078399663\n",
      "\n",
      "['his tax president has u fbi american up as against old the sea after –']\n",
      "Epoch 1700  ;  Discriminator loss : 0.4933483600616455   ;    Generator loss : 1.27507352828979573\n",
      "\n",
      "['the the on back media law against transition right not on with says kurdish is']\n",
      "Epoch 1800  ;  Discriminator loss : 0.49753621220588684   ;    Generator loss : 1.2675081491470337\n",
      "\n",
      "[\"of on in north and his rep be not with will for is for syria's\"]\n",
      "Epoch 1900  ;  Discriminator loss : 0.4696158766746521   ;    Generator loss : 1.33060359954833985\n",
      "\n",
      "['in trump trump in video jr trump u who in is u kenya for at']\n",
      "Epoch 2000  ;  Discriminator loss : 0.5002326965332031   ;    Generator loss : 1.31812226772308353\n",
      "\n",
      "['cuba political not ” says video on u the puts on over u ban house a']\n",
      "Epoch 2100  ;  Discriminator loss : 0.5526050925254822   ;    Generator loss : 1.17671525478363044\n",
      "\n",
      "[\"in house of senate sends trump a s trump trump to of to '\"]\n",
      "Epoch 2200  ;  Discriminator loss : 0.499242901802063   ;    Generator loss : 1.311485409736633331\n",
      "\n",
      "['video over repeal it’s full of campaign after the on trump to video trump ck']\n",
      "Epoch 2300  ;  Discriminator loss : 0.4897887706756592   ;    Generator loss : 1.43116176128387453\n",
      "\n",
      "['trump in from for u karma trump to of']\n",
      "Epoch 2400  ;  Discriminator loss : 0.49187880754470825   ;    Generator loss : 1.4562714099884033\n",
      "\n",
      "['to trump says says a libya of in obama trump trump to trump to ben']\n",
      "Epoch 2500  ;  Discriminator loss : 0.47953248023986816   ;    Generator loss : 1.3880320787429813\n",
      "\n",
      "['says watch source presidential obama it after video still have this in u of and men']\n",
      "Epoch 2600  ;  Discriminator loss : 0.4780610501766205   ;    Generator loss : 1.48070788383483897\n",
      "\n",
      "['for the after very video at for trump by breaking speaks video or in korea in']\n",
      "Epoch 2700  ;  Discriminator loss : 0.49131256341934204   ;    Generator loss : 1.5131481885910034\n",
      "\n",
      "['to trump video u his chris says u at president video of of u hariri']\n",
      "Epoch 2800  ;  Discriminator loss : 0.5021512508392334   ;    Generator loss : 1.50051450729370127\n",
      "\n",
      "['for u trump in statement with by from for nuclear to pm video strikes another to']\n",
      "Epoch 2900  ;  Discriminator loss : 0.5148866772651672   ;    Generator loss : 1.54884433746337912\n",
      "\n",
      "['to trump to to visit for is potential of on to twitter to state pro']\n",
      "Epoch 3000  ;  Discriminator loss : 0.5075504183769226   ;    Generator loss : 1.62461268901824951\n",
      "\n",
      "['trump trump to to revealed in house uk video video arms u with']\n",
      "Epoch 3100  ;  Discriminator loss : 0.5119725465774536   ;    Generator loss : 1.73268961906433157\n",
      "\n",
      "['to in to in this trump reveal trump to video to trump']\n",
      "Epoch 3200  ;  Discriminator loss : 0.4421318769454956   ;    Generator loss : 1.88298118114471442\n",
      "\n",
      "['of in over as to trump at to by irs for trump and trump clinton']\n",
      "Epoch 3300  ;  Discriminator loss : 0.46013930439949036   ;    Generator loss : 1.8362113237380981\n",
      "\n",
      "['american to the says to to trump to friends for in']\n",
      "Epoch 3400  ;  Discriminator loss : 0.5096601247787476   ;    Generator loss : 1.73003935813903861\n",
      "\n",
      "['while in obama clinton of trump of in storm to and trump the to']\n",
      "Epoch 3500  ;  Discriminator loss : 0.5128543376922607   ;    Generator loss : 1.84124171733856242\n",
      "\n",
      "['to for clinton and leftists with to trump being video']\n",
      "Epoch 3600  ;  Discriminator loss : 0.47941750288009644   ;    Generator loss : 1.7364079952239993\n",
      "\n",
      "['to to to the from on parliament the future video in on four and']\n",
      "Epoch 3700  ;  Discriminator loss : 0.4320877194404602   ;    Generator loss : 1.88072085380554275\n",
      "\n",
      "['trump militants in video on to a for after for trump sick']\n",
      "Epoch 3800  ;  Discriminator loss : 0.45819756388664246   ;    Generator loss : 1.9394350051879883\n",
      "\n",
      "[\"trump like in trump to a and for may's to just\"]\n",
      "Epoch 3900  ;  Discriminator loss : 0.45997101068496704   ;    Generator loss : 1.8509995937347412\n",
      "\n",
      "['to in video his ‘the from will obama global police will s a court state to']\n",
      "Epoch 4000  ;  Discriminator loss : 0.4355636239051819   ;    Generator loss : 1.69050705432891852\n",
      "\n",
      "['to in is make video second on as lied to are as with']\n",
      "Epoch 4100  ;  Discriminator loss : 0.4694638252258301   ;    Generator loss : 1.78133404254913333\n",
      "\n",
      "['to s s in under video talks trump for u at islamist']\n",
      "Epoch 4200  ;  Discriminator loss : 0.4825076460838318   ;    Generator loss : 1.63850319385528564\n",
      "\n",
      "['in in with hillary republicans ted 1 hillary national justice north gop up two muslims to']\n",
      "Epoch 4300  ;  Discriminator loss : 0.45267558097839355   ;    Generator loss : 1.6622668504714966\n",
      "\n",
      "['the trump racist into in obama’s u trump campaign out debt party street in this']\n",
      "Epoch 4400  ;  Discriminator loss : 0.486141562461853   ;    Generator loss : 1.594866037368774424\n",
      "\n",
      "['to to at hillary is a corruption s vote is news bomb u video part']\n",
      "Epoch 4500  ;  Discriminator loss : 0.44316035509109497   ;    Generator loss : 1.7277467250823975\n",
      "\n",
      "['trump to a about house and anti killed his the big armed at u republican']\n",
      "Epoch 4600  ;  Discriminator loss : 0.46442240476608276   ;    Generator loss : 1.7322928905487068\n",
      "\n",
      "[\"the video and economic while for the on the not by for video u trump '\"]\n",
      "Epoch 4700  ;  Discriminator loss : 0.47594892978668213   ;    Generator loss : 1.6247971057891846\n",
      "\n",
      "['trump to his of at may after issues his reason bill over hurricane her watch to']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4800  ;  Discriminator loss : 0.42816436290740967   ;    Generator loss : 1.5953807830810547\n",
      "\n",
      "['to to u in over his parliament rules healthcare against you bernie voters obama world']\n",
      "Epoch 4900  ;  Discriminator loss : 0.4689466953277588   ;    Generator loss : 1.61586081981658943\n",
      "\n",
      "['video trump hillary says news secretary election up this crazy fox watch black ahead you video']\n",
      "Epoch 5000  ;  Discriminator loss : 0.4447019100189209   ;    Generator loss : 1.74902367591857984\n",
      "\n",
      "['this trump funds watch her video video in s a over trump u s to tucker']\n",
      "Epoch 5100  ;  Discriminator loss : 0.44322994351387024   ;    Generator loss : 1.5858870744705202\n",
      "\n",
      "['to to a video and is but twitter ted says one gop migrant about who trump']\n",
      "Epoch 5200  ;  Discriminator loss : 0.4558199942111969   ;    Generator loss : 1.51682257652282714\n",
      "\n",
      "['to to for on obama about black israel pro black german former gop he four to']\n",
      "Epoch 5300  ;  Discriminator loss : 0.45707112550735474   ;    Generator loss : 1.5685197114944458\n",
      "\n",
      "['of in about it court america black uk gets amid does donald it another news for']\n",
      "Epoch 5400  ;  Discriminator loss : 0.4468787610530853   ;    Generator loss : 1.61756432056427866\n",
      "\n",
      "['on for her one you is will at after made sex american needs s one video']\n",
      "Epoch 5500  ;  Discriminator loss : 0.43286746740341187   ;    Generator loss : 1.9028080701828003\n",
      "\n",
      "['video of on on as it halt court u it trump and state']\n",
      "Epoch 5600  ;  Discriminator loss : 0.430899441242218   ;    Generator loss : 1.605970621109008896\n",
      "\n",
      "['to to they court after rally us muslim keep lives office about tweets not gets to']\n",
      "Epoch 5700  ;  Discriminator loss : 0.4602557420730591   ;    Generator loss : 1.68603491783142156\n",
      "\n",
      "['to on trump in and what as is calls state nuclear are for legislation']\n",
      "Epoch 5800  ;  Discriminator loss : 0.40908098220825195   ;    Generator loss : 1.6556938886642456\n",
      "\n",
      "['to to u for from as over against california senate anti sarah with ready will']\n",
      "Epoch 5900  ;  Discriminator loss : 0.45170778036117554   ;    Generator loss : 1.6022312641143799\n",
      "\n",
      "['to video state not it supreme probe shows because what may out macron her pro in']\n",
      "Epoch 6000  ;  Discriminator loss : 0.4347744286060333   ;    Generator loss : 1.65118396282196046\n",
      "\n",
      "['in the at white are be year jerusalem mass for by is is watch the free']\n",
      "Epoch 6100  ;  Discriminator loss : 0.43911921977996826   ;    Generator loss : 1.6226141452789307\n",
      "\n",
      "['trump in for for in for year at be from lied french senate at by of']\n",
      "Epoch 6200  ;  Discriminator loss : 0.44658735394477844   ;    Generator loss : 1.6172829866409302\n",
      "\n",
      "['in in video to says is u at made at is gets their countries']\n",
      "Epoch 6300  ;  Discriminator loss : 0.4589815139770508   ;    Generator loss : 1.62174248695373541\n",
      "\n",
      "['to trump for as u have has probe terrorist new blames plan how year law to']\n",
      "Epoch 6400  ;  Discriminator loss : 0.45354562997817993   ;    Generator loss : 1.7542439699172974\n",
      "\n",
      "['of the on new the what about from state plan security judge why picture police trump']\n",
      "Epoch 6500  ;  Discriminator loss : 0.4380594491958618   ;    Generator loss : 1.85508918762207036\n",
      "\n",
      "[\"in in trump a for president and after trump's with not trump’s the harry\"]\n",
      "Epoch 6600  ;  Discriminator loss : 0.4155083894729614   ;    Generator loss : 1.90102839469909678\n",
      "\n",
      "['to trump trump trump is as special been criminal video house s is']\n",
      "Epoch 6700  ;  Discriminator loss : 0.4186975359916687   ;    Generator loss : 1.88407909870147766\n",
      "\n",
      "['video u russia he’s bill his state my as backed cnn']\n",
      "Epoch 6800  ;  Discriminator loss : 0.4057397246360779   ;    Generator loss : 1.92045092582702645\n",
      "\n",
      "['trump trump in with was after ‘the black police from message trump’s cop']\n",
      "Epoch 6900  ;  Discriminator loss : 0.36743736267089844   ;    Generator loss : 2.0521230697631836\n",
      "\n",
      "['to to in from how be camp s obama s lives by tax']\n",
      "Epoch 7000  ;  Discriminator loss : 0.39719218015670776   ;    Generator loss : 1.9940774440765384\n",
      "\n",
      "['to to to house that huge fox republican about with 1 with fires tax']\n",
      "Epoch 7100  ;  Discriminator loss : 0.4369411766529083   ;    Generator loss : 1.91035354137420658\n",
      "\n",
      "['s open off donald into against trump’s his vote if poll change puerto senate time to']\n",
      "Epoch 7200  ;  Discriminator loss : 0.3927973508834839   ;    Generator loss : 2.13428878784179734\n",
      "\n",
      "['to trump to to in in have about after gop his islamist be they']\n",
      "Epoch 7300  ;  Discriminator loss : 0.4006361961364746   ;    Generator loss : 2.11321854591369632\n",
      "\n",
      "['to trump to his at obama friends of who as a russia']\n",
      "Epoch 7400  ;  Discriminator loss : 0.41266971826553345   ;    Generator loss : 1.9797570705413818\n",
      "\n",
      "['to video u may sarah media saudi abortion after watch by on be 5']\n",
      "Epoch 7500  ;  Discriminator loss : 0.41505229473114014   ;    Generator loss : 2.2129604816436768\n",
      "\n",
      "['to to to on trump is people for russian breaking zika about with republicans']\n",
      "Epoch 7600  ;  Discriminator loss : 0.42947500944137573   ;    Generator loss : 2.2456245422363288\n",
      "\n",
      "['for for after says gold in anti a up not him says u be the s']\n",
      "Epoch 7700  ;  Discriminator loss : 0.3963038921356201   ;    Generator loss : 2.23784279823303227\n",
      "\n",
      "['to to to a a in not obama and over a that decide']\n",
      "Epoch 7800  ;  Discriminator loss : 0.42444586753845215   ;    Generator loss : 2.2119264602661133\n",
      "\n",
      "['in in of 7 million don’t out cnn n still into election have']\n",
      "Epoch 7900  ;  Discriminator loss : 0.4057827293872833   ;    Generator loss : 2.12258172035217357\n",
      "\n",
      "['to to for u u president obama’s that report was loses paris white new her']\n",
      "Epoch 8000  ;  Discriminator loss : 0.39145857095718384   ;    Generator loss : 2.3571932315826416\n",
      "\n",
      "['to trump trump him s will with white is obama a by 24']\n",
      "Epoch 8100  ;  Discriminator loss : 0.4368065595626831   ;    Generator loss : 2.12981963157653863\n",
      "\n",
      "['for video about republican group the mcconnell gop racist can lives watch just china by for']\n",
      "Epoch 8200  ;  Discriminator loss : 0.39855077862739563   ;    Generator loss : 2.2196028232574463\n",
      "\n",
      "['trump in to video video from against seen says says racist gop his exposed']\n",
      "Epoch 8300  ;  Discriminator loss : 0.39636844396591187   ;    Generator loss : 2.3609256744384766\n",
      "\n",
      "['to to in the by source plan and be who he sea just']\n",
      "Epoch 8400  ;  Discriminator loss : 0.4061107635498047   ;    Generator loss : 2.15767598152160645\n",
      "\n",
      "['to trump to in court all aid may pm two senate crazy chief off']\n",
      "Epoch 8500  ;  Discriminator loss : 0.38885998725891113   ;    Generator loss : 2.1390120983123784\n",
      "\n",
      "['to video video says president groups up britain it anti war will from far']\n",
      "Epoch 8600  ;  Discriminator loss : 0.4174923598766327   ;    Generator loss : 2.69318222999572753\n",
      "\n",
      "['debate healthcare really back was video u house government video state senate says the with perfect']\n",
      "Epoch 8700  ;  Discriminator loss : 0.42238354682922363   ;    Generator loss : 2.0678255558013916\n",
      "\n",
      "['with of and s says judge air how real bill sanders top go this he move']\n",
      "Epoch 8800  ;  Discriminator loss : 0.37884414196014404   ;    Generator loss : 2.2437937259674072\n",
      "\n",
      "['in why thursday are not obamacare a police about over hillary’s']\n",
      "Epoch 8900  ;  Discriminator loss : 0.4061216711997986   ;    Generator loss : 2.23937678337097174\n",
      "\n",
      "['trump is with young after eu companies trump’s as bill over']\n",
      "Epoch 9000  ;  Discriminator loss : 0.4489874243736267   ;    Generator loss : 2.14552521705627447\n",
      "\n",
      "['to to trump trump trump puts after white as bernie year bernie korean trump’s election trump']\n",
      "Epoch 9100  ;  Discriminator loss : 0.39984017610549927   ;    Generator loss : 2.4044358730316164\n",
      "\n",
      "['trump trump video in to over in video watch says may will dialogue video is trump']\n",
      "Epoch 9200  ;  Discriminator loss : 0.38047897815704346   ;    Generator loss : 2.3251855373382576\n",
      "\n",
      "[\"to trump senator for his hillary of lebanon's says his u against him\"]\n",
      "Epoch 9300  ;  Discriminator loss : 0.4030529856681824   ;    Generator loss : 2.27028632164001464\n",
      "\n",
      "['video video of with political muslims tweets against wants new white are oops']\n",
      "Epoch 9400  ;  Discriminator loss : 0.41599416732788086   ;    Generator loss : 2.1863305568695074\n",
      "\n",
      "['trump trump after video for u congress tax pick white muslim act open ban ryan']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9444  ;  Discriminator loss : 0.3643667995929718   ;    Generator loss : 2.40490221977233997\r"
     ]
    }
   ],
   "source": [
    "DL, GL = train(generator, discriminator, gan, 15000, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_points = generate_latent_points(latent_dim, 10)\n",
    "predictions = generator.predict(few_points)\n",
    "\n",
    "print(predictions[3])\n",
    "\n",
    "#predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "print(predictions[3])\n",
    "\n",
    "print(tokenizer.sequences_to_texts(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# art plastique du turfu featuring le poto matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(1, 2)\n",
    "figure.set_size_inches(20,10)\n",
    "\n",
    "ax[0].plot(DL)\n",
    "\n",
    "ax[1].plot(GL)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = dataframe.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = to_predict[\"title\"]\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = to_predict[\"label\"]\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save(\"generator.h5\")\n",
    "discriminator.save(\"discriminator.h5\")\n",
    "tokenizerJSON = tokenizer.to_json()\n",
    "\n",
    "with open(\"gan_tokenizer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(tokenizerJSON, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
