{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras import Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, Conv1D, Dropout, Dense, Embedding, Flatten, Reshape, Multiply, Lambda, UpSampling1D\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Traitement des données</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement des deux tables\n",
    "real_news = pd.read_csv(\"./dataset1/True.csv\")\n",
    "fake_news = pd.read_csv(\"./dataset1/Fake.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vrai = 1, faux = 0\n",
    "real_news[\"label\"] = 1\n",
    "fake_news[\"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18089</th>\n",
       "      <td>WATCH SOLAR ECLIPSE LIVE HERE</td>\n",
       "      <td>If you don t have the proper glasses to watch ...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Aug 21, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7242</th>\n",
       "      <td>Over 60% of Republican Voters Are Embarrassed...</td>\n",
       "      <td>In an article aptly titled,  Why some Republic...</td>\n",
       "      <td>News</td>\n",
       "      <td>March 28, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3833</th>\n",
       "      <td>Russia election hacking a top global threat: D...</td>\n",
       "      <td>WASHINGTON (Reuters) - Russia’s attempts to in...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>May 11, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20524</th>\n",
       "      <td>BLACK CONSERVATIVE Student DESTROYS Black Live...</td>\n",
       "      <td>If you have the time, you should watch every m...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>May 25, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2577</th>\n",
       "      <td>As U.S. weighs Afghan strategy, hopes set on f...</td>\n",
       "      <td>KABUL (Reuters) - (This version of the July 23...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>July 23, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9459</th>\n",
       "      <td>PRESIDENT TRUMP Makes Room Erupt in Laughter w...</td>\n",
       "      <td>President Trump spoke at the State Banquet dur...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Nov 11, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17861</th>\n",
       "      <td>WHY ANTI-TRUMP BILLIONAIRE MARK CUBAN Couldn’t...</td>\n",
       "      <td>During the 2016 presidential election, Mark Cu...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Oct 4, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10960</th>\n",
       "      <td>WHY LET THE ATHEISTS Run All Over You? City Re...</td>\n",
       "      <td>A city in Pennsylvania is removing a park benc...</td>\n",
       "      <td>politics</td>\n",
       "      <td>May 6, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16748</th>\n",
       "      <td>YPG fighters credit Ocalan with Syria victory</td>\n",
       "      <td>BEIRUT (Reuters) - Fighters with the Syrian Ku...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>October 23, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11241</th>\n",
       "      <td>BOOM! SEAN SPICER: “Trump Sold Hotels In Russi...</td>\n",
       "      <td></td>\n",
       "      <td>politics</td>\n",
       "      <td>Mar 31, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "18089                      WATCH SOLAR ECLIPSE LIVE HERE   \n",
       "7242    Over 60% of Republican Voters Are Embarrassed...   \n",
       "3833   Russia election hacking a top global threat: D...   \n",
       "20524  BLACK CONSERVATIVE Student DESTROYS Black Live...   \n",
       "2577   As U.S. weighs Afghan strategy, hopes set on f...   \n",
       "9459   PRESIDENT TRUMP Makes Room Erupt in Laughter w...   \n",
       "17861  WHY ANTI-TRUMP BILLIONAIRE MARK CUBAN Couldn’t...   \n",
       "10960  WHY LET THE ATHEISTS Run All Over You? City Re...   \n",
       "16748      YPG fighters credit Ocalan with Syria victory   \n",
       "11241  BOOM! SEAN SPICER: “Trump Sold Hotels In Russi...   \n",
       "\n",
       "                                                    text       subject  \\\n",
       "18089  If you don t have the proper glasses to watch ...     left-news   \n",
       "7242   In an article aptly titled,  Why some Republic...          News   \n",
       "3833   WASHINGTON (Reuters) - Russia’s attempts to in...  politicsNews   \n",
       "20524  If you have the time, you should watch every m...     left-news   \n",
       "2577   KABUL (Reuters) - (This version of the July 23...  politicsNews   \n",
       "9459   President Trump spoke at the State Banquet dur...      politics   \n",
       "17861  During the 2016 presidential election, Mark Cu...     left-news   \n",
       "10960  A city in Pennsylvania is removing a park benc...      politics   \n",
       "16748  BEIRUT (Reuters) - Fighters with the Syrian Ku...     worldnews   \n",
       "11241                                                         politics   \n",
       "\n",
       "                    date  label  \n",
       "18089       Aug 21, 2017      0  \n",
       "7242      March 28, 2016      0  \n",
       "3833       May 11, 2017       1  \n",
       "20524       May 25, 2016      0  \n",
       "2577      July 23, 2017       1  \n",
       "9459        Nov 11, 2017      0  \n",
       "17861        Oct 4, 2017      0  \n",
       "10960        May 6, 2017      0  \n",
       "16748  October 23, 2017       1  \n",
       "11241       Mar 31, 2017      0  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# création du dataset complet\n",
    "dataframe = pd.concat([real_news, fake_news])\n",
    "dataframe.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de références : 44898\n",
      "Nombre de fake news : 23481\n",
      "Nombre de vraies news : 21417\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nombre de références : {dataframe.title.count()}\")\n",
    "print(f\"Nombre de fake news : {fake_news.title.count()}\")\n",
    "print(f\"Nombre de vraies news : {real_news.title.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ici on ne s'intéresse qu'au titre et au label\n",
    "del dataframe[\"title\"]\n",
    "del dataframe[\"subject\"]\n",
    "del dataframe[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6156</th>\n",
       "      <td>When Donald Trump recently said he would love ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11595</th>\n",
       "      <td>PARIS (Reuters) - France will on Thursday anno...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17338</th>\n",
       "      <td>LONDON (Reuters) - The probability that Britai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3983</th>\n",
       "      <td>WASHINGTON (Reuters) - An extra 2,500 visas fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11585</th>\n",
       "      <td>WASHINGTON (Reuters) - Russia is likely to mai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16036</th>\n",
       "      <td>Are there any Trump donors on the team? GREAT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2920</th>\n",
       "      <td>What happens when the most vile, sexist chauvi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18609</th>\n",
       "      <td>BARCELONA (Reuters) - Spanish national police ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3526</th>\n",
       "      <td>Ever since Matt Lauer threw Donald Trump some ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7829</th>\n",
       "      <td>(Reuters) - Republican presidential nominee Do...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "6156   When Donald Trump recently said he would love ...      0\n",
       "11595  PARIS (Reuters) - France will on Thursday anno...      1\n",
       "17338  LONDON (Reuters) - The probability that Britai...      1\n",
       "3983   WASHINGTON (Reuters) - An extra 2,500 visas fo...      1\n",
       "11585  WASHINGTON (Reuters) - Russia is likely to mai...      1\n",
       "16036   Are there any Trump donors on the team? GREAT...      0\n",
       "2920   What happens when the most vile, sexist chauvi...      0\n",
       "18609  BARCELONA (Reuters) - Spanish national police ...      1\n",
       "3526   Ever since Matt Lauer threw Donald Trump some ...      0\n",
       "7829   (Reuters) - Republican presidential nominee Do...      1"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nettoyage des données (ponctuations)\n",
    "\n",
    "#stopWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "def cleanText(text):\n",
    "    forbidden = {\",\",\"@\",\";\",\"/\",\"-\",\":\",\".\",\"!\",\"?\", \"#\",\"\\\"\",\"(\",\")\",\"\\'\",\"’\",\"‘\",\"–\",\"...\",\"&\"}\n",
    "    res = text\n",
    "    if res != None:\n",
    "        for elm in forbidden:\n",
    "            res = res.replace(elm, \"\")\n",
    "    if len(res.split()) >= 30:\n",
    "        res = \" \".join(res.split()[0:30])\n",
    "    if res != None:\n",
    "        for elm in forbidden:\n",
    "            res = res.replace(\"  \", \" \")\n",
    "    return res\n",
    "\n",
    "dataframe[\"text\"] = dataframe[\"text\"].apply(cleanText)\n",
    "\n",
    "#dataframe.sample(10)\n",
    "cleanText(\"test...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'milita...',\n",
       " 'Vassil...',\n",
       " 'larger',\n",
       " 'mouthpiece',\n",
       " 'Easter',\n",
       " 'Prospects',\n",
       " 'East...',\n",
       " 'Xi...',\n",
       " 'bridg...',\n",
       " 'AB',\n",
       " 'Sat...',\n",
       " '$120',\n",
       " 'felon...',\n",
       " 'Democratic',\n",
       " 'wat...',\n",
       " 'tv',\n",
       " 'Kwambana',\n",
       " 'chi...',\n",
       " 'economically',\n",
       " 'armoured',\n",
       " 'Comma...',\n",
       " 'exaggerate',\n",
       " 'Dakotas',\n",
       " 'DMI',\n",
       " 'fed...',\n",
       " 'ABORTION',\n",
       " 'oneApparentl...',\n",
       " 'aggressive...',\n",
       " 'regi...',\n",
       " 'Pastafarianism',\n",
       " 'maj...',\n",
       " 'shortly',\n",
       " 'DIEGO',\n",
       " 'forfeiture',\n",
       " 'PRESIDENT',\n",
       " 'Illin...',\n",
       " 'Nusra',\n",
       " 'lobby...',\n",
       " 'Hassett',\n",
       " 'minor',\n",
       " 'DELHITOKYO',\n",
       " 'Venezuelan',\n",
       " 'sucks',\n",
       " 'whack',\n",
       " 'Handwriting',\n",
       " 'Fina...',\n",
       " 'Bol...',\n",
       " 'Kentuck...',\n",
       " 'logic',\n",
       " 'Mi8',\n",
       " 'ravings',\n",
       " 'Fordham',\n",
       " 'fed',\n",
       " 'sole',\n",
       " 'Twice',\n",
       " 'globe',\n",
       " 'Centu...',\n",
       " 'buildin...',\n",
       " 'subtl...',\n",
       " 'proposa...',\n",
       " 'COLBERT',\n",
       " 'qualification',\n",
       " 'Job...',\n",
       " 'documentary',\n",
       " 'babble',\n",
       " 'shamefully',\n",
       " 'establish...',\n",
       " 'Jone...',\n",
       " 'harassed',\n",
       " 'embar...',\n",
       " 'hereDuring',\n",
       " 'explorer',\n",
       " 'Pepe',\n",
       " 'smack...',\n",
       " 'Lemonis',\n",
       " 'emphasis',\n",
       " 'July',\n",
       " 'privilege',\n",
       " 'grateful',\n",
       " 'suspend...',\n",
       " 'grand',\n",
       " 'disrespec...',\n",
       " 'HAPPENS',\n",
       " 'hangs',\n",
       " 'intercepted',\n",
       " 'tons',\n",
       " 'MARSEILLE',\n",
       " 'teenage',\n",
       " 'creating',\n",
       " 'altercation',\n",
       " 'holiday',\n",
       " 'album',\n",
       " 'minionGOP',\n",
       " 'cancelled',\n",
       " 'failures',\n",
       " 'superintendent',\n",
       " 'CLEARLY',\n",
       " 'Lisa',\n",
       " 'unscripted',\n",
       " 'healt...',\n",
       " 'Congolese',\n",
       " 'goon',\n",
       " 'solution',\n",
       " 'promoting',\n",
       " 'Bangladesh',\n",
       " 'Rogowsky',\n",
       " 'track',\n",
       " 'ter',\n",
       " 'strategizing',\n",
       " 'Transportation...',\n",
       " 'IngrahamAngle',\n",
       " 'Joint',\n",
       " 'telling',\n",
       " 'Way',\n",
       " 'ext...',\n",
       " 'ALL...',\n",
       " 'graduating',\n",
       " 'tide',\n",
       " 'disclosure',\n",
       " 'worms',\n",
       " 'securit...',\n",
       " 'prof...',\n",
       " 'BOZANOVICI',\n",
       " 'Anglophone',\n",
       " 'Jonathon',\n",
       " 'withdraw',\n",
       " 'uni...',\n",
       " 'Audience',\n",
       " 'MISMANAGE...',\n",
       " 'prank',\n",
       " 'confe...',\n",
       " 'LOVE...',\n",
       " 'containing',\n",
       " 'recusal',\n",
       " 'times...',\n",
       " 'performer',\n",
       " 'Andrey',\n",
       " 'BOGOTACARTAGENA',\n",
       " 'Kaine',\n",
       " 'proced...',\n",
       " 'GENEVASHAMLAPUR',\n",
       " 'charged...',\n",
       " 'Hedge',\n",
       " 'personality',\n",
       " 'confirms',\n",
       " 'DEA...',\n",
       " 'Sirico',\n",
       " 'sue...',\n",
       " 'Col...',\n",
       " 'celebr...',\n",
       " 'Especially',\n",
       " 'increased',\n",
       " 'EXTRADITION',\n",
       " 'scope',\n",
       " 'Jaein',\n",
       " 'responsibil...',\n",
       " 'opponent',\n",
       " 'Child',\n",
       " '10minute...',\n",
       " 'Alber...',\n",
       " 'Rating',\n",
       " 'ci...',\n",
       " 'militia',\n",
       " 'viral',\n",
       " 'trip...',\n",
       " 'Trollin...',\n",
       " 'residen...',\n",
       " 'broken',\n",
       " 'Patriotism',\n",
       " 'targeting...',\n",
       " 'Welsh',\n",
       " 'pledged...',\n",
       " 'HAGUE',\n",
       " 'Lit...',\n",
       " 'Freddie',\n",
       " 'WOODBRIDGE',\n",
       " 'out',\n",
       " 'sha...',\n",
       " 'balan...',\n",
       " 'candidateTrump',\n",
       " 'DonaldTrump',\n",
       " 'Zarr...',\n",
       " 'At...',\n",
       " 'housing',\n",
       " 'dividing...',\n",
       " '85yea...',\n",
       " 'Shame',\n",
       " 'SUBIC',\n",
       " 'Sena...',\n",
       " 'Sergei...',\n",
       " 'Canada',\n",
       " 'Council',\n",
       " 'Fein',\n",
       " 'AntiTrumper',\n",
       " 'Vi...',\n",
       " 'HOPE',\n",
       " 'SENATOR',\n",
       " 'refusing',\n",
       " 'Demonstrations',\n",
       " 'freaking',\n",
       " 'Cup',\n",
       " 'shotTh...',\n",
       " 'Diehard',\n",
       " 'arrogance',\n",
       " 'Mums',\n",
       " 'Jr',\n",
       " 'birdies',\n",
       " 'catching...',\n",
       " 'Stachowiak',\n",
       " 'Moscows',\n",
       " 'sites',\n",
       " 'promoted',\n",
       " 'walked',\n",
       " 'illegals',\n",
       " 'Marilyn',\n",
       " 'Malawian',\n",
       " 'Dario',\n",
       " 'logi...',\n",
       " 'Rights',\n",
       " 'GIVING',\n",
       " 'ava...',\n",
       " 'muchdela...',\n",
       " 'displayin...',\n",
       " 'Presidential...',\n",
       " 'Eric',\n",
       " 'scores',\n",
       " '730',\n",
       " 'morons',\n",
       " 'prev...',\n",
       " 'Investigators',\n",
       " 'Cape',\n",
       " 'skipping',\n",
       " 'FBI...',\n",
       " 'Mic...',\n",
       " 'them...',\n",
       " 'adjunct',\n",
       " 'emai...',\n",
       " 'Photos',\n",
       " 'Rakhmat',\n",
       " 'Schilling',\n",
       " 'inse...',\n",
       " 'directly',\n",
       " 'behind...',\n",
       " 'constructive',\n",
       " 'Peo...',\n",
       " 'Broadly',\n",
       " 'heart...',\n",
       " 'prog...',\n",
       " 'Accountabilit...',\n",
       " 'stamps',\n",
       " 'Garrison',\n",
       " '$10000',\n",
       " 'hospitals',\n",
       " 'YANGONSHAH',\n",
       " 'raised',\n",
       " 'Warren',\n",
       " 'embraced',\n",
       " 'specu...',\n",
       " 'crackdow...',\n",
       " 'Leader',\n",
       " 'entrepreneur',\n",
       " 'the...',\n",
       " 'Afghani...',\n",
       " 'PEDDLING',\n",
       " 'Filipe',\n",
       " 'upstanding',\n",
       " 'Stallon...',\n",
       " 'coy',\n",
       " 'Bill...',\n",
       " 'cocaine',\n",
       " 'whether',\n",
       " 'o...',\n",
       " 'bidding',\n",
       " 'Maury',\n",
       " 'treati...',\n",
       " 'ramps...',\n",
       " 'slimy...',\n",
       " 'Yea',\n",
       " 'illega...',\n",
       " 'Vitiello',\n",
       " 'inherited',\n",
       " 'speculati...',\n",
       " 'separation',\n",
       " 'CRISTOBALMARACAIBO',\n",
       " 'Dice',\n",
       " 'Munich',\n",
       " 'pac...',\n",
       " 'let...',\n",
       " 'brouhaha',\n",
       " 'NJ',\n",
       " 'UN...',\n",
       " 'McCains',\n",
       " 'Antiabortion',\n",
       " 'unilateral',\n",
       " 'lays',\n",
       " 'spoke...',\n",
       " 'Lab...',\n",
       " 'Hegseth',\n",
       " 'Lili',\n",
       " 'Margaret',\n",
       " 'buddies',\n",
       " 'laundering',\n",
       " 'Lancer',\n",
       " 'Mad...',\n",
       " 'col...',\n",
       " '159',\n",
       " 'extra',\n",
       " 'Criminal',\n",
       " 'Delegates',\n",
       " 'Boxy',\n",
       " 'SOCHI',\n",
       " 'Sometime',\n",
       " 'ones',\n",
       " 'FSB',\n",
       " 'desc...',\n",
       " 'KUALA',\n",
       " 'investi...',\n",
       " 'wage...',\n",
       " 'showdown',\n",
       " 'leadin...',\n",
       " '5',\n",
       " 'kicks',\n",
       " 'ahea...',\n",
       " 'Edelman',\n",
       " 'Ingraham',\n",
       " 'skewed',\n",
       " 'welcomes',\n",
       " 'screening',\n",
       " 'strong',\n",
       " 'Women',\n",
       " 'Cameras',\n",
       " 'vid...',\n",
       " 'Domingo',\n",
       " 'Never',\n",
       " 'whopping',\n",
       " 'INSANEIn',\n",
       " 'taxpaying',\n",
       " 'mysterious',\n",
       " 'Z',\n",
       " 'Force...',\n",
       " 'covers',\n",
       " 'Bernish',\n",
       " 'reducti...',\n",
       " 'Yiannopoulos...',\n",
       " 'RUSSIA',\n",
       " 'above',\n",
       " 'countr...',\n",
       " 'university',\n",
       " 'ma...',\n",
       " 'unexpectedly...',\n",
       " 'lik...',\n",
       " 'atmosphere',\n",
       " 'Lacey',\n",
       " 'Austral...',\n",
       " 'Schwarzenegger',\n",
       " 'DeF...',\n",
       " 'result',\n",
       " 'unveiled',\n",
       " 'tattoo',\n",
       " 'She',\n",
       " 'Ilia',\n",
       " 'Klan',\n",
       " 'Clear',\n",
       " 'RColorado',\n",
       " 'umpteenth',\n",
       " 'param...',\n",
       " 'banks',\n",
       " 'travelling',\n",
       " 'explosives',\n",
       " 'Zeke',\n",
       " 'kangaroo',\n",
       " 'critics',\n",
       " 'courts',\n",
       " 'OhioWASHINGTON',\n",
       " 'Depart...',\n",
       " 'Brien',\n",
       " 'infidelity',\n",
       " 'investo...',\n",
       " 'embattled',\n",
       " 'Whedon',\n",
       " 'oh',\n",
       " 'immune',\n",
       " 'mix',\n",
       " 'solutions',\n",
       " 'Everyday',\n",
       " 'unbelie...',\n",
       " 'BoomDear',\n",
       " 'Ha...',\n",
       " 'secr...',\n",
       " 'UNbacked',\n",
       " 'sacked',\n",
       " 'SHOUT',\n",
       " 'Gutfield',\n",
       " 'fantastic',\n",
       " 'phrase',\n",
       " 'jokeBlackLivesMatter',\n",
       " 'BROWNSVILLE',\n",
       " 'Transcanada',\n",
       " '203',\n",
       " 'ongoin...',\n",
       " 'assistance',\n",
       " 'signing',\n",
       " 'Texas...',\n",
       " 'Karodia...',\n",
       " 'PJ...',\n",
       " 'wonWhy...',\n",
       " 'arrangement',\n",
       " 'vot...',\n",
       " 'oth...',\n",
       " 'post',\n",
       " 'Congressm...',\n",
       " 'trolli...',\n",
       " 'sitin',\n",
       " 'BLOEMFONTEIN',\n",
       " 'rub...',\n",
       " 'OTTAWATORONTO',\n",
       " 'Earnest',\n",
       " 'Martinique',\n",
       " 'declare',\n",
       " 'Subway',\n",
       " 'inmates...',\n",
       " 'Devin',\n",
       " 'Classified',\n",
       " 'environment',\n",
       " 'elimi...',\n",
       " 'anywher...',\n",
       " 'observed',\n",
       " 'Relations...',\n",
       " 'sen...',\n",
       " 'trai...',\n",
       " 'highway',\n",
       " 'Catalon...',\n",
       " '$43',\n",
       " 'list...',\n",
       " 'Cord...',\n",
       " 'pressured',\n",
       " 'dest...',\n",
       " 'observation',\n",
       " 'gripe',\n",
       " 'Luxembourg',\n",
       " 'TOAST',\n",
       " 'square',\n",
       " 'swear',\n",
       " 'scie...',\n",
       " 'swimmer',\n",
       " 'runoff',\n",
       " 'paratroopers',\n",
       " 'kettle',\n",
       " 'Glasgow',\n",
       " 'STATEMENT',\n",
       " 'costliest',\n",
       " 'estimates',\n",
       " 'coach',\n",
       " 'Studies',\n",
       " 'captivating',\n",
       " 'which...',\n",
       " 'contestan...',\n",
       " 'Nathan',\n",
       " 'photographs',\n",
       " 'academic',\n",
       " 'overthr...',\n",
       " '41yearold',\n",
       " 'hyp...',\n",
       " 'Pundit',\n",
       " 'honoring',\n",
       " 'Marxist...',\n",
       " 'overturning',\n",
       " 'fear',\n",
       " 'acciden...',\n",
       " 'Bettors',\n",
       " 'Clyburn',\n",
       " 'LOS',\n",
       " 'endorsing',\n",
       " 'hur...',\n",
       " 'Final',\n",
       " 'ahead',\n",
       " 'Yusuf',\n",
       " 'fake...',\n",
       " 'Health',\n",
       " 'segments',\n",
       " 'Pardoning',\n",
       " 'disrespected',\n",
       " 'pack',\n",
       " 'Congressman',\n",
       " 'except',\n",
       " 'Ryans...',\n",
       " 'Pentagons',\n",
       " 'governorate',\n",
       " 'son...',\n",
       " 'protests...',\n",
       " 'rebounded...',\n",
       " 'GOPer',\n",
       " 'exploit',\n",
       " 'bribed',\n",
       " 'manwoman',\n",
       " 'GAZAJERUSALEM',\n",
       " 'Spicey',\n",
       " 'alternative',\n",
       " 'cars',\n",
       " 'donat...',\n",
       " 'hypocrite',\n",
       " 'Arabs',\n",
       " 'starters',\n",
       " 'Hoping',\n",
       " 'ALLOWED',\n",
       " 'Canadi...',\n",
       " 'bump',\n",
       " 'httpswwwyoutubecomwatchv=YeDU6dCR9tA',\n",
       " 'trail',\n",
       " 'posit...',\n",
       " 'Sit',\n",
       " 'disclosed',\n",
       " 'pattern',\n",
       " 'honor...',\n",
       " 'regis...',\n",
       " 'Curnell',\n",
       " 'blo...',\n",
       " 'economic',\n",
       " 'slowly',\n",
       " 'accompli...',\n",
       " 'Hezbollah',\n",
       " 'Fourteen',\n",
       " 'Kredo',\n",
       " 'espionage',\n",
       " 'deflect',\n",
       " 'manag...',\n",
       " 'politicians',\n",
       " 'warriors',\n",
       " 'Pennsylvania',\n",
       " 'Prevent',\n",
       " 'closel...',\n",
       " 'DELETED',\n",
       " 'Angola',\n",
       " 'Civilians',\n",
       " 'WAY',\n",
       " 'Menzio',\n",
       " 'Hot',\n",
       " 'whethe...',\n",
       " 'apologi...',\n",
       " 'Zoo...',\n",
       " 'nah',\n",
       " 'REMEMBER',\n",
       " 'leftlea...',\n",
       " 'Intell...',\n",
       " 'arrived',\n",
       " 'Vikt...',\n",
       " 'exci...',\n",
       " 'lost...',\n",
       " 'glowi...',\n",
       " 'Residents',\n",
       " 'newlyelected',\n",
       " 'governin...',\n",
       " 'Alinsky',\n",
       " 'throughout',\n",
       " 'Gun',\n",
       " 'defens...',\n",
       " 'ROMEPARIS',\n",
       " 'Mead...',\n",
       " 'learne...',\n",
       " 'OdomBell',\n",
       " 'CATANO',\n",
       " 'COPS',\n",
       " 'syndrome',\n",
       " 'volunt...',\n",
       " 'people',\n",
       " 'terminal',\n",
       " 'spies',\n",
       " 'Mili...',\n",
       " '$212',\n",
       " 'nearby',\n",
       " 'smoke',\n",
       " 'critic',\n",
       " 'unending',\n",
       " 'aptly',\n",
       " 'ISI...',\n",
       " 'Warring',\n",
       " 'requi...',\n",
       " 'parliamen...',\n",
       " 'uphel...',\n",
       " 'creationist',\n",
       " 'corps...',\n",
       " 'excerpt',\n",
       " 'photographer...',\n",
       " 'teeming',\n",
       " 'commit',\n",
       " 'recording',\n",
       " 'tremendously',\n",
       " 'Taxpolicy',\n",
       " 'suit',\n",
       " 'SREBRENICA',\n",
       " 'hospi...',\n",
       " 'minority',\n",
       " 'publishing',\n",
       " 'nut',\n",
       " 'vacancy',\n",
       " 'Pi...',\n",
       " 'beneficiary',\n",
       " 'primar...',\n",
       " 'ALWAYS',\n",
       " 'bombi...',\n",
       " 'DIGENOVA',\n",
       " 'Stephanopo...',\n",
       " 'welcomed',\n",
       " 'vanquished',\n",
       " 'resort',\n",
       " 'Dr',\n",
       " 'intelligence...',\n",
       " 'mainstrea...',\n",
       " 'CAIRO',\n",
       " 'pundits',\n",
       " 'SIAULIAI',\n",
       " 'httpswwwyoutubecomwatchtime_continue=2v=IjWClQ...',\n",
       " 'Irm...',\n",
       " 'Asse...',\n",
       " 'rigged',\n",
       " 'ithe',\n",
       " 'disturb...',\n",
       " 'integrate',\n",
       " 'Christies',\n",
       " 'ther...',\n",
       " 'preciou...',\n",
       " 'Parks',\n",
       " 'GABORONE',\n",
       " 'Nicholas',\n",
       " 'cours...',\n",
       " 'sandwich',\n",
       " 'Hernandez',\n",
       " 'scion',\n",
       " 'Bankruptcies',\n",
       " 'httpswwwyoutubecomwatchv=31MRqr9ydUU',\n",
       " 'juggernaut',\n",
       " 'probability',\n",
       " 'receivi...',\n",
       " 'Palestinians...',\n",
       " 'Leandra',\n",
       " 'dems',\n",
       " 'razorsha...',\n",
       " 'governmentbacked',\n",
       " 'swallo...',\n",
       " 'Socialist...',\n",
       " 'preside...',\n",
       " 'httpswwwyoutubecomwatchv=PTBfkqk7gU',\n",
       " 'asyl...',\n",
       " 'Luttrell',\n",
       " 'Celebrities',\n",
       " 'Standing',\n",
       " 'Awkwardly',\n",
       " 'Logan',\n",
       " 'LUCEDALE',\n",
       " 'Governme...',\n",
       " 'deep',\n",
       " 'Teresa',\n",
       " 'Newfoundland',\n",
       " 'KIEVCHISINAU',\n",
       " 'reportHillary',\n",
       " 'gover...',\n",
       " 'Share',\n",
       " 'BRAVO',\n",
       " 'secular',\n",
       " 'Srinivasan',\n",
       " 'Department',\n",
       " 'CommanderinChief',\n",
       " 'that...',\n",
       " 'evergreen',\n",
       " 'richer',\n",
       " 'robotic',\n",
       " 'grifters',\n",
       " 'gentleman',\n",
       " 'candid',\n",
       " 'waiting',\n",
       " 'Pet...',\n",
       " 'Creepy',\n",
       " 'battled',\n",
       " 'baggag...',\n",
       " 'matt...',\n",
       " 'EVERY',\n",
       " 'Jacob...',\n",
       " 'Overlooked',\n",
       " 'Dana',\n",
       " 'antitrust',\n",
       " 'wallets',\n",
       " 'Review',\n",
       " 'Hooper',\n",
       " 'Diane',\n",
       " 'BERLINGREIFSWALD',\n",
       " 'presumptiv...',\n",
       " 'putz',\n",
       " 'mili...',\n",
       " 'WETUMPKA',\n",
       " 'Missile',\n",
       " 'Ash...',\n",
       " 'ticked',\n",
       " 'enterpris...',\n",
       " 'Streisand',\n",
       " 'Forefront',\n",
       " 'rephrase',\n",
       " 'PROTECTION...',\n",
       " '172',\n",
       " 'Negotiators',\n",
       " 'hurry',\n",
       " 'Flashback',\n",
       " 'trad...',\n",
       " 'CEDAR',\n",
       " 'inspir...',\n",
       " 'B',\n",
       " 'du...',\n",
       " 'REFERENCES',\n",
       " 'retir...',\n",
       " 'limit...',\n",
       " 'citizens',\n",
       " 'guards',\n",
       " 'entry',\n",
       " 'formerly',\n",
       " 'EXAMPLE',\n",
       " 'clever',\n",
       " 'Salman',\n",
       " 'centrists',\n",
       " 'Around',\n",
       " 'Raj...',\n",
       " 'idiots...',\n",
       " 'cookie',\n",
       " 'MANHOOD',\n",
       " 'viewpoint...',\n",
       " 'JR',\n",
       " 'var',\n",
       " 'SQUATTERS',\n",
       " 'saw',\n",
       " 'Guido',\n",
       " 'cit...',\n",
       " 'dumbest',\n",
       " 'exporters',\n",
       " 'Lind...',\n",
       " 'Zeal...',\n",
       " 'suc...',\n",
       " 'Darfur',\n",
       " 'Longer',\n",
       " 'Almost',\n",
       " 'bridges...',\n",
       " 'emp...',\n",
       " 'China',\n",
       " 'educated',\n",
       " 'Spices',\n",
       " 'rain',\n",
       " 'YANGONBANGKOK',\n",
       " 'downAs',\n",
       " 'Ju...',\n",
       " 'MAGA3X',\n",
       " '23yea...',\n",
       " 'SHAREIf',\n",
       " 'Sauli',\n",
       " '716',\n",
       " 'auto...',\n",
       " 'fundamental',\n",
       " 'Chester',\n",
       " 'becom...',\n",
       " 'Bangladesh...',\n",
       " 'Hi...',\n",
       " 'legislative',\n",
       " 'AfricanA...',\n",
       " 'Trumpbashing',\n",
       " 'Vt',\n",
       " 'agree',\n",
       " 'hell',\n",
       " 'Appropriation...',\n",
       " 'newsp...',\n",
       " 'great...',\n",
       " 'paves',\n",
       " 'Mack',\n",
       " 'Stock',\n",
       " 'hel...',\n",
       " 'hat',\n",
       " 'GATEWAY',\n",
       " 'Starbucks',\n",
       " 'supporters',\n",
       " 'Hans',\n",
       " 'fleeing',\n",
       " 'Douglas',\n",
       " 'Affai...',\n",
       " 'GADSDEN',\n",
       " 'Resettlement',\n",
       " 'Jones',\n",
       " 'Ah...',\n",
       " 'sout...',\n",
       " 'rights...',\n",
       " 'dicta...',\n",
       " 'presi...',\n",
       " 'Ultra',\n",
       " 'narcissis...',\n",
       " 'Mafia',\n",
       " 'canonized',\n",
       " 'nurs...',\n",
       " 'Use',\n",
       " 'Communists',\n",
       " 'screen',\n",
       " 'nonsensical',\n",
       " 'millennials',\n",
       " 'defe...',\n",
       " 'patri...',\n",
       " 'TODAY',\n",
       " 'rebranded',\n",
       " 'embarked',\n",
       " 'word',\n",
       " 'indicte...',\n",
       " 'ANAHEIM',\n",
       " 'club',\n",
       " 'LITERALLY',\n",
       " 'pai...',\n",
       " 'Tide',\n",
       " 'constituents',\n",
       " 'offer',\n",
       " 'Providence',\n",
       " 'Divided',\n",
       " 'Gent...',\n",
       " 'oppose...',\n",
       " 'Northam',\n",
       " 'pilotless...',\n",
       " 'nasty',\n",
       " 'detecte...',\n",
       " 'Stoynoff',\n",
       " 'Moonbeam',\n",
       " 'notic...',\n",
       " 'norm...',\n",
       " 'KNEW',\n",
       " 'impeaching',\n",
       " 'repulsive',\n",
       " 'Lee...',\n",
       " 'Panama',\n",
       " 'BRUTALLY',\n",
       " 'jumped',\n",
       " 'SES...',\n",
       " 'WAYNESBURG',\n",
       " 'Pavlop...',\n",
       " 'Stayer',\n",
       " 'partie...',\n",
       " 'nightmarish',\n",
       " 'rampant',\n",
       " 'accompanied',\n",
       " 'Bouthaina',\n",
       " 'diagnosed',\n",
       " 'conferen...',\n",
       " 'MARIB',\n",
       " 'SOMALIA',\n",
       " 'baseball',\n",
       " 'unhinged',\n",
       " 'Assembly',\n",
       " 'rei...',\n",
       " 'BUDAPESTWARSAW',\n",
       " 'Assa...',\n",
       " 'featured...',\n",
       " 'erroneously',\n",
       " 'Murray',\n",
       " 'models',\n",
       " 'Indi...',\n",
       " 'hanged',\n",
       " 'Marshall',\n",
       " 'Athens',\n",
       " 'neck',\n",
       " 'Yulin',\n",
       " 'Mevlut...',\n",
       " 'doneI',\n",
       " 'Pelos...',\n",
       " 'VOLUME***These',\n",
       " 'Cassidy',\n",
       " 'assurances...',\n",
       " 'WASHINGTONLOS',\n",
       " 'sta...',\n",
       " 'condemns',\n",
       " 'dredger...',\n",
       " 'killing',\n",
       " 'traveling',\n",
       " 'kind',\n",
       " 'Conference',\n",
       " 'SAN',\n",
       " 'Angeles',\n",
       " 'Uppsala',\n",
       " 'representatives',\n",
       " 'OnlyNC',\n",
       " 'Truck',\n",
       " 'YMCA',\n",
       " 'corrupt',\n",
       " 'conference',\n",
       " 'these',\n",
       " 'blessed',\n",
       " 'steak',\n",
       " 'Beebe',\n",
       " 'Jesu...',\n",
       " 'Fiat',\n",
       " 'live',\n",
       " 'Rafeal',\n",
       " 'rapist...',\n",
       " 'foc...',\n",
       " 'Europ...',\n",
       " 'Mast',\n",
       " 'disturbing',\n",
       " 'Saudiowned',\n",
       " 'property',\n",
       " 'scrap...',\n",
       " 'lives',\n",
       " 'Kong',\n",
       " 'steroids',\n",
       " 'Edouard',\n",
       " 'YORK',\n",
       " 'technician',\n",
       " 'Rosen',\n",
       " 'EXCLUSIVE',\n",
       " 'vete...',\n",
       " 'Jeffer...',\n",
       " 'Kudlow...',\n",
       " 'sa...',\n",
       " 'showcase',\n",
       " 'pric...',\n",
       " 'LEE',\n",
       " 'Being',\n",
       " 'MANILA',\n",
       " 'Cameroonian',\n",
       " 'Protectio...',\n",
       " 'Clay',\n",
       " 'surprising',\n",
       " 'south',\n",
       " 'vigorous...',\n",
       " 'Trumpcare',\n",
       " 'Aurora',\n",
       " 'tightknit',\n",
       " 'Esteemed',\n",
       " 'Maureen',\n",
       " 'Celebrity',\n",
       " 'Pathetic',\n",
       " 'corrupt...',\n",
       " 'Random',\n",
       " 'envisioned',\n",
       " 'bloc',\n",
       " 'Tolerance',\n",
       " 'nev...',\n",
       " 'improvised',\n",
       " 'giant',\n",
       " 'Maggie',\n",
       " 'compl...',\n",
       " 'suffer...',\n",
       " 'Gundlach',\n",
       " 'Uruguay',\n",
       " 'beauti...',\n",
       " 'quive...',\n",
       " 'nod',\n",
       " 'exhausted',\n",
       " 'httpswwwyoutubecomwatchtime_continue=1v=NeqMSI...',\n",
       " 'TUNISROME',\n",
       " 'LONDON',\n",
       " 'hoped...',\n",
       " 'rife',\n",
       " 'mic...',\n",
       " 'LIST',\n",
       " 'lowers',\n",
       " 'cosmetic',\n",
       " 'wanted...',\n",
       " 'INSULTING...',\n",
       " 'allimportant',\n",
       " 'Loret...',\n",
       " 'THAN...',\n",
       " 'Cop',\n",
       " 'Supporting',\n",
       " 'requests',\n",
       " 'attempt...',\n",
       " 'Irma',\n",
       " 'Han...',\n",
       " 'habit',\n",
       " 'Gottlieb',\n",
       " 'GB',\n",
       " 'bulletriddled',\n",
       " 'Factory',\n",
       " 'prosecutor...',\n",
       " 'Shavenheaded',\n",
       " 'WASHINGTONST',\n",
       " 'mayoral',\n",
       " 'miracles',\n",
       " 'South...',\n",
       " 'during...',\n",
       " 'Almo...',\n",
       " '2466',\n",
       " 'Chandler',\n",
       " 'absolu...',\n",
       " 'Dominion',\n",
       " 'wrought',\n",
       " 'stunned',\n",
       " 'selffunding',\n",
       " 'Sworn',\n",
       " 'BRANCHBURG',\n",
       " 'sending',\n",
       " 'reportedCalifornia',\n",
       " 'Grim',\n",
       " 'depl...',\n",
       " 'LUXEMBOURGBRUSSELS',\n",
       " 'welldes...',\n",
       " 'speec...',\n",
       " 'frightening',\n",
       " 'Fi...',\n",
       " 'Relat...',\n",
       " 'colle...',\n",
       " 'bannin...',\n",
       " 'MichKENT',\n",
       " 'rememb...',\n",
       " ...}"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataframe[\"text\"].to_string(index=False).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données d'entrainement : 40408\n",
      "Données de test : 4490\n"
     ]
    }
   ],
   "source": [
    "# on sépare les données en données d'entraînement et données de test (80% et 20%)\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataframe[\"text\"], dataframe[\"label\"], test_size=0.10, random_state = 42)\n",
    "print(f\"Données d'entrainement : {len(x_train)}\")\n",
    "print(f\"Données de test : {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000 # taille max du vocab\n",
    "maxlen = 30 # taille max de séquence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorisation naïve en \"one-hot\"\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorisation des données d'entraînement\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorisation des données de test\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "nb_epochs = 20\n",
    "embedded_dim = 100\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, nbpoints):\n",
    "    return np.random.uniform(0, 1, size=[nbpoints, latent_dim])\n",
    "\n",
    "def getLatentSamples(latent_dim, n):\n",
    "    labels = np.zeros(shape=n)\n",
    "    samples = generate_latent_points(latent_dim, n)\n",
    "    \n",
    "    return samples, labels\n",
    "\n",
    "def getFakeSamples(generator, latent_dim, n):\n",
    "    labels = np.ones(shape=n)\n",
    "    \n",
    "    latent_points = generate_latent_points(latent_dim, n)\n",
    "    samples = generator.predict(latent_points)\n",
    "    \n",
    "    return samples, labels\n",
    "\n",
    "def getRealSamples(X, Y, n):\n",
    "    random_indices = np.random.randint(0, X.shape[0], n)\n",
    "\n",
    "    samples = X[random_indices]\n",
    "    labels = Y[random_indices]\n",
    "    \n",
    "    return samples, labels\n",
    "\n",
    "def generateFakeNews(model, n):\n",
    "    few_points = generate_latent_points(latent_dim, n)\n",
    "    predictions = generator.predict(few_points)\n",
    "    fake_news = tokenizer.sequences_to_texts(np.round(predictions))\n",
    "    \n",
    "    return fake_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(dim):\n",
    "    \n",
    "    input_layer = Input(shape=[dim])\n",
    "    \n",
    "    x = Dense(maxlen, input_shape=[dim])(input_layer)\n",
    "    \n",
    "    x = Reshape((maxlen, 1))(x)\n",
    "    x = Conv1D(128, 3, padding=\"same\")(x)\n",
    "    x = UpSampling1D()(x)\n",
    "    x = Conv1D(64, 3, padding=\"same\")(x)\n",
    "    x = UpSampling1D()(x)\n",
    "    x = Conv1D(32, 3, padding=\"same\")(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(maxlen, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    output_layer = Lambda(lambda x: x * float(max_features))(x)\n",
    "\n",
    "    model = Model(input_layer, output_layer)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.0002, beta_1=.5))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_discriminator():\n",
    "\n",
    "    input_layer = Input(shape=[maxlen])\n",
    "    \n",
    "    #x = Embedding(max_features, output_dim=embedded_dim, input_length=maxlen, trainable=True, input_shape=[maxlen])(input_layer)\n",
    "    x = Dense(embedded_dim * maxlen)(input_layer)\n",
    "    x = Reshape((maxlen, embedded_dim))(x)\n",
    "    x = Conv1D(64, 3, padding=\"same\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Conv1D(32, 3, padding=\"same\")(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    output_layer = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.0002, beta_1=.5))\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_gan(generator, discriminator, latent_dim):\n",
    "    \n",
    "    input_layer = Input(shape=[latent_dim])\n",
    "    \n",
    "    x = generator(input_layer)\n",
    "    \n",
    "    output_layer = discriminator(x)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.0002, beta_1=.5))\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_34 (InputLayer)        (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 3000)              93000     \n",
      "_________________________________________________________________\n",
      "reshape_23 (Reshape)         (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 30, 64)            19264     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 30, 32)            6176      \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 960)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 961       \n",
      "=================================================================\n",
      "Total params: 119,401\n",
      "Trainable params: 119,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_35 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "reshape_24 (Reshape)         (None, 30, 1)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 30, 128)           512       \n",
      "_________________________________________________________________\n",
      "up_sampling1d_19 (UpSampling (None, 60, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 60, 64)            24640     \n",
      "_________________________________________________________________\n",
      "up_sampling1d_20 (UpSampling (None, 120, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 120, 32)           6176      \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 3840)              0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 30)                115230    \n",
      "_________________________________________________________________\n",
      "lambda_10 (Lambda)           (None, 30)                0         \n",
      "=================================================================\n",
      "Total params: 149,588\n",
      "Trainable params: 149,588\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_36 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "model_31 (Model)             (None, 30)                149588    \n",
      "_________________________________________________________________\n",
      "model_30 (Model)             (None, 1)                 119401    \n",
      "=================================================================\n",
      "Total params: 268,989\n",
      "Trainable params: 268,989\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = create_discriminator()\n",
    "generator = create_generator(latent_dim)\n",
    "gan = create_gan(generator, discriminator, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator_model, discriminator_model, gan_model, nb_epochs, batch_size):\n",
    "    discriminator_losses = []\n",
    "    \n",
    "    generator_losses = []\n",
    "    \n",
    "    for i in range(nb_epochs):\n",
    "\n",
    "        x_fake, y_fake = getFakeSamples(generator_model, latent_dim, batch_size)\n",
    "        x_real, y_real = getRealSamples(x_train, y_train, batch_size)\n",
    "        \n",
    "        discriminator_model.trainable = True\n",
    "\n",
    "        loss1 = discriminator_model.train_on_batch(x_real, y_real)\n",
    "        loss2 = discriminator_model.train_on_batch(x_fake, y_fake)\n",
    "        loss3 = (loss1 + loss2) / 2\n",
    "        \n",
    "        discriminator_model.trainable = False\n",
    "        \n",
    "        x_gan, y_gan = getLatentSamples(latent_dim, batch_size)        \n",
    "        \n",
    "        loss_gan = gan_model.train_on_batch(x_gan, y_gan)\n",
    "\n",
    "        discriminator_losses.append(loss3)\n",
    "        \n",
    "        generator_losses.append(loss_gan)\n",
    "        \n",
    "        print(f\"Discriminator loss : {loss3}   ;    Generator loss : {loss_gan}\", end=\"\\r\")\n",
    "        if i % 1000 == 0:\n",
    "            print(generateFakeNews(generator_model, 1))\n",
    "        \n",
    "    return discriminator_losses, generator_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shadow\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fjsparentnodeinsertbeforejs deadlines streisand hiroshima firefighters emphasize pile bells patience forfeiture scotus glowing refinery friendship excuses ridehailing erupt swear reorganization chained vendors alberta blockbuster miner scenarios controlling cheney beholden blanket secondlargest']\n",
      "['the decided many could week house reuters on as is us for of in for on trump that he reuters in trump that in and reuters as s that reuters']\n",
      "['the investigation left century him be to to his it with in of reuters reuters a donald of donald on on that on on of of washington in of trump']\n",
      "['would should president been a to the on on to of the the a the to to of a the to a to to the of to a a']\n",
      "['first announced washington trump on to to of trump on of to a in a of of of in to a in to on a on a of of']\n",
      "['during donald of tax to to the of a to to the the to to to a to to the to to a to to the to to the']\n",
      "['s before on to on the the the the the the the the the the the the the the the the the the the the to the']\n",
      "['he']minator loss : 53.447731018066406   ;    Generator loss : 1.888803124427795466\n",
      "['says']nator loss : 325.4255676269531   ;    Generator loss : 1.335057497024536177755\n",
      "['only']nator loss : 144.3647918701172   ;    Generator loss : 0.001753527787514031974\n",
      "['off']inator loss : 166.96974182128906   ;    Generator loss : 1.73815774917602548569\n",
      "['iraq']nator loss : 151.57183837890625   ;    Generator loss : 5.607176376543066e-075\n",
      "['long']nator loss : 150.21041870117188   ;    Generator loss : 3.8212638173718005e-06\n",
      "['freedom']or loss : 116.36298370361328   ;    Generator loss : 0.30176278948783875808\n",
      "['los and']or loss : 61.84345245361328   ;    Generator loss : 5.269401550292969249365\n",
      "['meanwhile the']s : 52.73007583618164   ;    Generator loss : 0.557993948459625266747\n",
      "['women']ator loss : 26.48247718811035   ;    Generator loss : 0.2208867371082306154\n",
      "['national']r loss : 15.893611907958984   ;    Generator loss : 0.18028849363327026\n",
      "['most']nator loss : 10.629545211791992   ;    Generator loss : 0.17198994755744934\n",
      "['against']or loss : 8.140792846679688   ;    Generator loss : 0.140664458274841382\n",
      "['to']minator loss : 4.759477615356445   ;    Generator loss : 0.083090119063854226\n",
      "['on']minator loss : 4.647347450256348   ;    Generator loss : 0.0620945058763027286\n",
      "['what']nator loss : 3.689469814300537   ;    Generator loss : 0.0396399572491645855\n",
      "['']riminator loss : 3.896284818649292   ;    Generator loss : 0.0262924805283546454\n",
      "['']riminator loss : 2.7944045066833496   ;    Generator loss : 0.021763389930129056\n",
      "['the']inator loss : 3.225801944732666   ;    Generator loss : 0.0159367881715297753\n",
      "['']riminator loss : 2.8945348262786865   ;    Generator loss : 0.012130716815590858\n",
      "['the']inator loss : 3.090838670730591   ;    Generator loss : 0.0100854467600584038\n",
      "['']riminator loss : 3.267704963684082   ;    Generator loss : 0.0089566465467214585\n",
      "['']riminator loss : 3.2686171531677246   ;    Generator loss : 0.007899357005953789\n",
      "['twitter to']loss : 17.196638107299805   ;    Generator loss : 0.0012868531048297882\n",
      "['']riminator loss : 18.518449783325195   ;    Generator loss : 0.00355477328412234825\n",
      "['']riminator loss : 29.361631393432617   ;    Generator loss : 0.0026265960186719894\n",
      "['']riminator loss : 42.3244743347168   ;    Generator loss : 0.001907876925542950643\n",
      "['']riminator loss : 43.29773712158203   ;    Generator loss : 0.00152989523485302931\n",
      "['']riminator loss : 45.270511627197266   ;    Generator loss : 0.0006854681414552033\n",
      "['']riminator loss : 93.15713500976562   ;    Generator loss : 0.000497343891765922375\n",
      "['']riminator loss : 83.14514923095703   ;    Generator loss : 0.000372192735085263854\n",
      "['']riminator loss : 117.30440521240234   ;    Generator loss : 0.00030447711469605565\n",
      "['']riminator loss : 67.46955108642578   ;    Generator loss : 0.000247937685344368246\n",
      "['']riminator loss : 57.93556213378906   ;    Generator loss : 0.000169208738952875147\n",
      "['']riminator loss : 50.59051513671875   ;    Generator loss : 9.069729276234284e-0555\n",
      "['']riminator loss : 55.195770263671875   ;    Generator loss : 5.7037032092921436e-05\n",
      "['']riminator loss : 49.69462585449219   ;    Generator loss : 4.092470044270158e-0555\n",
      "['']riminator loss : 33.580116271972656   ;    Generator loss : 3.668188219307922e-055\n",
      "['']riminator loss : 40.85161590576172   ;    Generator loss : 3.346266021253541e-0555\n",
      "['']riminator loss : 51.90451431274414   ;    Generator loss : 2.684042374312412e-0555\n",
      "['']riminator loss : 36.021724700927734   ;    Generator loss : 3.5735090932575986e-05\n",
      "['']riminator loss : 28.037952423095703   ;    Generator loss : 3.762419510167092e-055\n",
      "['']riminator loss : 29.9306697845459   ;    Generator loss : 3.6861263652099296e-0555\n",
      "Discriminator loss : 1063.5477294921875   ;    Generator loss : 3.657962224679068e-055\r"
     ]
    }
   ],
   "source": [
    "DL, GL = train(generator, discriminator, gan, 50000, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_points = generate_latent_points(latent_dim, 10)\n",
    "predictions = generator.predict(few_points)\n",
    "#print(predictions)\n",
    "print(tokenizer.sequences_to_texts(np.round(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# art plastique du turfu featuring le poto matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(1, 2)\n",
    "figure.set_size_inches(20,10)\n",
    "\n",
    "ax[0].plot(DL)\n",
    "\n",
    "ax[1].plot(GL)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = dataframe.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = to_predict[\"title\"]\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = to_predict[\"label\"]\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save(\"generator.h5\")\n",
    "discriminator.save(\"discriminator.h5\")\n",
    "tokenizerJSON = tokenizer.to_json()\n",
    "\n",
    "with open(\"gan_tokenizer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(tokenizerJSON, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
