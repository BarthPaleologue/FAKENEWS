{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras import Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, Conv1D, Dropout, Dense, Embedding, Flatten, Reshape, Multiply, Lambda, UpSampling1D, MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Traitement des données</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement des deux tables\n",
    "real_news = pd.read_csv(\"./dataset1/True.csv\")\n",
    "fake_news = pd.read_csv(\"./dataset1/Fake.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vrai = 1, faux = 0\n",
    "real_news[\"label\"] = 1\n",
    "fake_news[\"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10950</th>\n",
       "      <td>Vice President Biden to go to Mexico City Feb....</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. Vice President Joe...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>February 5, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8771</th>\n",
       "      <td>Clinton says Trump is most divisive candidate ...</td>\n",
       "      <td>In a speech weighted with America’s complicate...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>July 13, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5897</th>\n",
       "      <td>Sudan summons U.S. charge d'affaires over Trum...</td>\n",
       "      <td>CAIRO (Reuters) - Sudan summoned the U.S. char...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>January 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>FBI officials said Clinton 'has to win' race t...</td>\n",
       "      <td>(Reuters) - Senior FBI officials who helped pr...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 13, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>Trump Lashes Out At Black CEO For Resigning F...</td>\n",
       "      <td>Donald Trump s stunning neglect to disavow the...</td>\n",
       "      <td>News</td>\n",
       "      <td>August 14, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18711</th>\n",
       "      <td>With Budapest closer to Moscow, Orban grants m...</td>\n",
       "      <td>ZALAVAR, Hungary (Reuters) - Twice a month a f...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>Trump Comes For McConnell AGAIN With Latest S...</td>\n",
       "      <td>Donald Trump seems to think that he can bully ...</td>\n",
       "      <td>News</td>\n",
       "      <td>August 10, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>A Florida Pizza Hut To Irma-Fleeing Employees...</td>\n",
       "      <td>Residents in Florida who work at a Pizza Hut w...</td>\n",
       "      <td>News</td>\n",
       "      <td>September 11, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19128</th>\n",
       "      <td>Qatar Foreign Minister: blockade pushing it cl...</td>\n",
       "      <td>PARIS (Reuters) - An economic blockade on Qata...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 25, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861</th>\n",
       "      <td>Trump Adviser Confirms: Trump’s Immigration P...</td>\n",
       "      <td>It s been a hell of a week for the Trump campa...</td>\n",
       "      <td>News</td>\n",
       "      <td>August 28, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "10950  Vice President Biden to go to Mexico City Feb....   \n",
       "8771   Clinton says Trump is most divisive candidate ...   \n",
       "5897   Sudan summons U.S. charge d'affaires over Trum...   \n",
       "188    FBI officials said Clinton 'has to win' race t...   \n",
       "569     Trump Lashes Out At Black CEO For Resigning F...   \n",
       "18711  With Budapest closer to Moscow, Orban grants m...   \n",
       "592     Trump Comes For McConnell AGAIN With Latest S...   \n",
       "395     A Florida Pizza Hut To Irma-Fleeing Employees...   \n",
       "19128  Qatar Foreign Minister: blockade pushing it cl...   \n",
       "4861    Trump Adviser Confirms: Trump’s Immigration P...   \n",
       "\n",
       "                                                    text       subject  \\\n",
       "10950  WASHINGTON (Reuters) - U.S. Vice President Joe...  politicsNews   \n",
       "8771   In a speech weighted with America’s complicate...  politicsNews   \n",
       "5897   CAIRO (Reuters) - Sudan summoned the U.S. char...  politicsNews   \n",
       "188    (Reuters) - Senior FBI officials who helped pr...  politicsNews   \n",
       "569    Donald Trump s stunning neglect to disavow the...          News   \n",
       "18711  ZALAVAR, Hungary (Reuters) - Twice a month a f...     worldnews   \n",
       "592    Donald Trump seems to think that he can bully ...          News   \n",
       "395    Residents in Florida who work at a Pizza Hut w...          News   \n",
       "19128  PARIS (Reuters) - An economic blockade on Qata...     worldnews   \n",
       "4861   It s been a hell of a week for the Trump campa...          News   \n",
       "\n",
       "                      date  label  \n",
       "10950    February 5, 2016       1  \n",
       "8771        July 13, 2016       1  \n",
       "5897     January 29, 2017       1  \n",
       "188     December 13, 2017       1  \n",
       "569        August 14, 2017      0  \n",
       "18711  September 29, 2017       1  \n",
       "592        August 10, 2017      0  \n",
       "395     September 11, 2017      0  \n",
       "19128  September 25, 2017       1  \n",
       "4861       August 28, 2016      0  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# création du dataset complet\n",
    "dataframe = pd.concat([real_news, fake_news])\n",
    "dataframe.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de références : 44898\n",
      "Nombre de fake news : 23481\n",
      "Nombre de vraies news : 21417\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nombre de références : {dataframe.title.count()}\")\n",
    "print(f\"Nombre de fake news : {fake_news.title.count()}\")\n",
    "print(f\"Nombre de vraies news : {real_news.title.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ici on ne s'intéresse qu'au titre et au label\n",
    "del dataframe[\"text\"]\n",
    "del dataframe[\"subject\"]\n",
    "del dataframe[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>House Intelligence chairman cleared of disclos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5760</th>\n",
       "      <td>Trump's EPA pick vote delayed in boycott by Se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11182</th>\n",
       "      <td>Immigration case could hurt Republican outreac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10364</th>\n",
       "      <td>WOMAN HOSPITALIZED, UNABLE TO FEEL HER LEGS Af...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15425</th>\n",
       "      <td>Two children killed as car crashes into Austra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21163</th>\n",
       "      <td>Muslim pilgrims converge on Jamarat for symbol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10978</th>\n",
       "      <td>RADICAL “TOLERANT” FEMALE Black Bloc, Antifa L...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18407</th>\n",
       "      <td>UNHINGED MIKA Called President Trump “Not Well...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7522</th>\n",
       "      <td>Trump Gets Snippy After Being Asked To Dial D...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2724</th>\n",
       "      <td>Threatening note left at senator's office amid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  label\n",
       "258    House Intelligence chairman cleared of disclos...      1\n",
       "5760   Trump's EPA pick vote delayed in boycott by Se...      1\n",
       "11182  Immigration case could hurt Republican outreac...      1\n",
       "10364  WOMAN HOSPITALIZED, UNABLE TO FEEL HER LEGS Af...      0\n",
       "15425  Two children killed as car crashes into Austra...      1\n",
       "21163  Muslim pilgrims converge on Jamarat for symbol...      1\n",
       "10978  RADICAL “TOLERANT” FEMALE Black Bloc, Antifa L...      0\n",
       "18407  UNHINGED MIKA Called President Trump “Not Well...      0\n",
       "7522    Trump Gets Snippy After Being Asked To Dial D...      0\n",
       "2724   Threatening note left at senator's office amid...      1"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nettoyage des données (ponctuations)\n",
    "\n",
    "#stopWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "def cleanText(text):\n",
    "    forbidden = {\",\",\"@\",\";\",\"/\",\"-\",\":\",\".\",\"!\",\"?\", \"#\",\"\\\"\",\"(\",\")\",\"\\'\",\"’\",\"‘\",\"–\",\".\",\"&\"}\n",
    "    res = str(text)\n",
    "    if res != None:\n",
    "        for elm in forbidden:\n",
    "            res = res.replace(elm, \"\")\n",
    "    if len(res.split()) >= 30:\n",
    "        res = \" \".join(res.split()[0:30])\n",
    "    if res != None:\n",
    "        for elm in forbidden:\n",
    "            res = res.replace(\"  \", \" \")\n",
    "    return res\n",
    "\n",
    "dataframe[\"title\"] = dataframe[\"title\"].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        As US budget fight looms Republicans flip thei...\n",
       "1        US military to accept transgender recruits on ...\n",
       "2        Senior US Republican senator Let Mr Mueller do...\n",
       "3        FBI Russia probe helped by Australian diplomat...\n",
       "4        Trump wants Postal Service to charge much more...\n",
       "                               ...                        \n",
       "23476    McPain John McCain Furious That Iran Treated U...\n",
       "23477    JUSTICE Yahoo Settles Email Privacy Classactio...\n",
       "23478    Sunnistan US and Allied Safe Zone Plan to Take...\n",
       "23479    How to Blow $700 Million Al Jazeera America Fi...\n",
       "23480    10 US Navy Sailors Held by Iranian Military Si...\n",
       "Name: title, Length: 44898, dtype: object"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données d'entrainement : 40408\n",
      "Données de test : 4490\n"
     ]
    }
   ],
   "source": [
    "# on sépare les données en données d'entraînement et données de test (80% et 20%)\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataframe[\"title\"], dataframe[\"label\"], test_size=0.10, random_state = 42)\n",
    "print(f\"Données d'entrainement : {len(x_train)}\")\n",
    "print(f\"Données de test : {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000 # taille max du vocab\n",
    "maxlen = 15 # taille max de séquence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorisation naïve en \"one-hot\"\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(dataframe[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorisation des données d'entraînement\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorisation des données de test\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "nb_epochs = 20\n",
    "embedded_dim = 100\n",
    "latent_dim = 100\n",
    "kernel_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, nbpoints):\n",
    "    return np.random.uniform(0, 1, size=[nbpoints, latent_dim])\n",
    "\n",
    "def getLatentSamples(latent_dim, n):\n",
    "    labels = np.zeros(shape=n)\n",
    "    samples = generate_latent_points(latent_dim, n)\n",
    "    \n",
    "    return samples, labels\n",
    "\n",
    "def getFakeSamples(generator, latent_dim, n):\n",
    "    labels = np.ones(shape=n)\n",
    "    \n",
    "    latent_points = generate_latent_points(latent_dim, n)\n",
    "    samples = generator.predict(latent_points)\n",
    "    \n",
    "    return samples, labels\n",
    "\n",
    "def getRealSamples(X, Y, n):\n",
    "    random_indices = np.random.randint(0, X.shape[0], n)\n",
    "\n",
    "    samples = X[random_indices]\n",
    "    labels = Y[random_indices]\n",
    "    \n",
    "    return samples, labels\n",
    "\n",
    "def generateFakeNews(model, n):\n",
    "    few_points = generate_latent_points(latent_dim, n)\n",
    "    predictions = generator.predict(few_points)\n",
    "    fake_news = tokenizer.sequences_to_texts(np.round(predictions))\n",
    "    \n",
    "    return fake_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(dim):\n",
    "    \n",
    "    input_layer = Input(shape=[dim])\n",
    "    \n",
    "    x = Dense(maxlen, input_shape=[dim])(input_layer)\n",
    "    \n",
    "    x = Reshape((maxlen, 1))(x)\n",
    "    x = Conv1D(128, kernel_size, padding=\"same\")(x)\n",
    "    x = UpSampling1D()(x)\n",
    "    x = Conv1D(64, kernel_size, padding=\"same\")(x)\n",
    "    x = UpSampling1D()(x)\n",
    "    x = Conv1D(32, kernel_size, padding=\"same\")(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(maxlen, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    output_layer = Lambda(lambda x: x * float(max_features))(x)\n",
    "\n",
    "    model = Model(input_layer, output_layer)\n",
    "    model.compile(loss=\"mse\", optimizer=Adam(lr=0.0002, beta_1=.5))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_discriminator():\n",
    "\n",
    "    input_layer = Input(shape=[maxlen])\n",
    "    \n",
    "    #x = Embedding(max_features, output_dim=embedded_dim, input_length=maxlen, trainable=True, input_shape=[maxlen])(input_layer)\n",
    "    x = Dense(embedded_dim * maxlen)(input_layer)\n",
    "    x = Reshape((maxlen, embedded_dim))(x)\n",
    "    x = Conv1D(128, kernel_size, padding=\"same\")(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Conv1D(64, kernel_size, padding=\"same\")(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Conv1D(32, kernel_size, padding=\"same\")(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    output_layer = Dense(1, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.0002, beta_1=.5))\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_gan(generator, discriminator, latent_dim):\n",
    "    \n",
    "    input_layer = Input(shape=[latent_dim])\n",
    "    \n",
    "    x = generator(input_layer)\n",
    "    \n",
    "    output_layer = discriminator(x)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.0002, beta_1=.5))\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1500)              24000     \n",
      "_________________________________________________________________\n",
      "reshape_19 (Reshape)         (None, 15, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 15, 128)           128128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 7, 64)             81984     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 3, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 3, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 3, 32)             20512     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 254,657\n",
      "Trainable params: 254,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 15)                1515      \n",
      "_________________________________________________________________\n",
      "reshape_20 (Reshape)         (None, 15, 1)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 15, 128)           1408      \n",
      "_________________________________________________________________\n",
      "up_sampling1d_19 (UpSampling (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 30, 64)            81984     \n",
      "_________________________________________________________________\n",
      "up_sampling1d_20 (UpSampling (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 60, 32)            20512     \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 15)                28815     \n",
      "_________________________________________________________________\n",
      "lambda_10 (Lambda)           (None, 15)                0         \n",
      "=================================================================\n",
      "Total params: 134,234\n",
      "Trainable params: 134,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_30 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "model_29 (Model)             (None, 15)                134234    \n",
      "_________________________________________________________________\n",
      "model_28 (Model)             (None, 1)                 254657    \n",
      "=================================================================\n",
      "Total params: 388,891\n",
      "Trainable params: 388,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = create_discriminator()\n",
    "generator = create_generator(latent_dim)\n",
    "gan = create_gan(generator, discriminator, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator_model, discriminator_model, gan_model, nb_epochs, batch_size):\n",
    "    discriminator_losses = []\n",
    "    \n",
    "    generator_losses = []\n",
    "    \n",
    "    for i in range(nb_epochs):\n",
    "\n",
    "        x_fake, y_fake = getFakeSamples(generator_model, latent_dim, batch_size)\n",
    "        x_real, y_real = getRealSamples(x_train, y_train, batch_size)\n",
    "        \n",
    "        discriminator_model.trainable = True\n",
    "\n",
    "        loss1 = discriminator_model.train_on_batch(x_real, y_real)\n",
    "        loss2 = discriminator_model.train_on_batch(x_fake, y_fake)\n",
    "        loss3 = (loss1 + loss2) / 2\n",
    "        \n",
    "        discriminator_model.trainable = False\n",
    "        \n",
    "        x_gan, y_gan = getLatentSamples(latent_dim, batch_size)        \n",
    "        \n",
    "        loss_gan = gan_model.train_on_batch(x_gan, y_gan)\n",
    "\n",
    "        discriminator_losses.append(loss3)\n",
    "        \n",
    "        generator_losses.append(loss_gan)\n",
    "        \n",
    "        print(f\"Epoch {i}  ;  Discriminator loss : {loss3}   ;    Generator loss : {loss_gan}\", end=\"\\r\")\n",
    "        if i % 100 == 0:\n",
    "            print(\"\\n\")\n",
    "            print(generateFakeNews(generator_model, 1))\n",
    "        \n",
    "    return discriminator_losses, generator_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shadow\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  ;  Discriminator loss : 35.32588577270508   ;    Generator loss : 1154.1905517578125\n",
      "\n",
      "['tribal learn toddler cultural tillersons dissidents isolated chiles awarded newest squad petty fireworks struggling advocate']\n",
      "Epoch 100  ;  Discriminator loss : 8.874604225158691   ;    Generator loss : 1.2377797365188599\n",
      "\n",
      "['room antitrump time she border ted cnn their turkey lawmakers nyc makes backs brexit hell']\n",
      "Epoch 200  ;  Discriminator loss : 4.3781633377075195   ;    Generator loss : 1.1954911947250366\n",
      "\n",
      "['into south army attack show plan military who gop america chief republican more wont wont']\n",
      "Epoch 300  ;  Discriminator loss : 5.002150535583496   ;    Generator loss : 1.20067250728607185\n",
      "\n",
      "['gop as for as trump the court trump us after at video to on in']\n",
      "Epoch 400  ;  Discriminator loss : 3.226269245147705   ;    Generator loss : 1.04040384292602546\n",
      "\n",
      "['they house is minister as with a and for with fbi video in on trump']\n",
      "Epoch 500  ;  Discriminator loss : 4.329286098480225   ;    Generator loss : 0.86657249927520757\n",
      "\n",
      "['south house from the the video in has to for in to to trump trump']\n",
      "Epoch 600  ;  Discriminator loss : 2.422818660736084   ;    Generator loss : 1.07927286624908456\n",
      "\n",
      "['video on at to trump in be to to']\n",
      "Epoch 700  ;  Discriminator loss : 2.3207077980041504   ;    Generator loss : 1.2615001201629639\n",
      "\n",
      "['will bill with for of their in trumps trump and in in to trump in']\n",
      "Epoch 800  ;  Discriminator loss : 1.6217708587646484   ;    Generator loss : 1.1658480167388916\n",
      "\n",
      "['white against and plan obama only after hillary says house with obama video and of']\n",
      "Epoch 900  ;  Discriminator loss : 4.558006286621094   ;    Generator loss : 0.82939982414245634\n",
      "\n",
      "['have for trump for of about gets for in video in as in to']\n",
      "Epoch 1000  ;  Discriminator loss : 2.3803606033325195   ;    Generator loss : 1.115297794342041\n",
      "\n",
      "['with in in us to trump plan trump in in to']\n",
      "Epoch 1100  ;  Discriminator loss : 1.6560018062591553   ;    Generator loss : 1.1109312772750854\n",
      "\n",
      "['trump of video video to of one trump to obama says to to']\n",
      "Epoch 1200  ;  Discriminator loss : 1.3104606866836548   ;    Generator loss : 0.9541344046592712\n",
      "\n",
      "['to watch to to trump to']\n",
      "Epoch 1210  ;  Discriminator loss : 0.8692313432693481   ;    Generator loss : 1.0361261367797852"
     ]
    }
   ],
   "source": [
    "DL, GL = train(generator, discriminator, gan, 50000, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_points = generate_latent_points(latent_dim, 10)\n",
    "predictions = generator.predict(few_points)\n",
    "#print(predictions)\n",
    "print(tokenizer.sequences_to_texts(np.round(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# art plastique du turfu featuring le poto matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(1, 2)\n",
    "figure.set_size_inches(20,10)\n",
    "\n",
    "ax[0].plot(DL)\n",
    "\n",
    "ax[1].plot(GL)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = dataframe.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = to_predict[\"title\"]\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = to_predict[\"label\"]\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save(\"generator.h5\")\n",
    "discriminator.save(\"discriminator.h5\")\n",
    "tokenizerJSON = tokenizer.to_json()\n",
    "\n",
    "with open(\"gan_tokenizer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(tokenizerJSON, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
